{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4795e6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "tensor = torch.rand(3,4, dtype= )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2d2c3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a52e791",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = torch.nn.Sequential( torch.nn.Linear(4, 3), torch.nn.ReLU(), torch.nn.Linear(3, 1) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bcb33c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=3, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=3, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e54a1ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.nn.Linear(4, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "be7de67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Simple perceptron\n",
    "class SimplePerceptron(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimplePerceptron, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.linear(x)\n",
    "        return torch.sigmoid(output)\n",
    "\n",
    "# Usage\n",
    "input_size = 2  # Number of input features\n",
    "\n",
    "# Initialize the model\n",
    "model = SimplePerceptron(input_size)\n",
    "\n",
    "# Example input (two features)\n",
    "x = torch.tensor([1.0, 1.0])\n",
    "\n",
    "# Get the output (prediction)\n",
    "output = model(x)\n",
    "\n",
    "# Print the output\n",
    "print(\"Output:\", output.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "143c9088",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x2 and 1x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model(torch\u001b[39m.\u001b[39;49mtensor([\u001b[39m1.0\u001b[39;49m, \u001b[39m1.0\u001b[39;49m]))\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[86], line 11\u001b[0m, in \u001b[0;36mSimplePerceptron.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 11\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear(x)\n\u001b[1;32m     12\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39msigmoid(output)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x2 and 1x2)"
     ]
    }
   ],
   "source": [
    "model(torch.tensor([1.0, 1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4d77c34d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x2 and 1x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model(torch\u001b[39m.\u001b[39;49mtensor([\u001b[39m1.0\u001b[39;49m, \u001b[39m1.0\u001b[39;49m]))\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[73], line 12\u001b[0m, in \u001b[0;36mSimplePerceptron.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 12\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear(x)\n\u001b[1;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39msigmoid(output)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x2 and 1x2)"
     ]
    }
   ],
   "source": [
    "model(torch.tensor([1.0, 1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ae14615a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2567ea7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " shape of X: torch.Size([5]), shape of y: torch.Size([3])\n",
      "tensor([0.3034], grad_fn=<SigmoidBackward0>) tensor([0.8823, 0.9150, 0.3829])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Simple perceptron\n",
    "class SimplePerceptron(nn.Module):\n",
    "    def __init__(self, input_size, out_size):\n",
    "        super(SimplePerceptron, self).__init__()\n",
    "        self.linear =  nn.Linear(input_size, out_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.linear(x)\n",
    "        return torch.sigmoid(output)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "input_size = 5\n",
    "out_size = 3\n",
    "learning_rate = 1\n",
    "epochs = 10000\n",
    "\n",
    "# Generate random data for training\n",
    "X = torch.rand(input_size)\n",
    "y = X[: out_size]\n",
    "#y_onehot = torch.zeros(size, 2).scatter_(1, y.unsqueeze(1), 1)\n",
    "\n",
    "# Model parameters\n",
    "print(f\" shape of X: {X.shape}, shape of y: {y.shape}\")\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimplePerceptron(input_size, out_size)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the loss for this epoch\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c5cd738e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8823, 0.9150, 0.3829])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "383832d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8823, 0.9150, 0.3829, 0.9593, 0.3904])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5b028ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5062, 0.6538, 0.4869], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "2b799597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X: torch.Size([100, 1]), shape of y: torch.Size([100, 1])\n",
      "Epoch [1/10000], Loss: 1312.6780\n",
      "Epoch [2/10000], Loss: 1278.6072\n",
      "Epoch [3/10000], Loss: 1250.1503\n",
      "Epoch [4/10000], Loss: 1222.4148\n",
      "Epoch [5/10000], Loss: 1189.6452\n",
      "Epoch [6/10000], Loss: 1153.5596\n",
      "Epoch [7/10000], Loss: 1119.8865\n",
      "Epoch [8/10000], Loss: 1088.2989\n",
      "Epoch [9/10000], Loss: 1057.6030\n",
      "Epoch [10/10000], Loss: 1025.9015\n",
      "Epoch [11/10000], Loss: 993.2218\n",
      "Epoch [12/10000], Loss: 958.0746\n",
      "Epoch [13/10000], Loss: 923.4249\n",
      "Epoch [14/10000], Loss: 889.7601\n",
      "Epoch [15/10000], Loss: 857.1247\n",
      "Epoch [16/10000], Loss: 825.2771\n",
      "Epoch [17/10000], Loss: 794.9078\n",
      "Epoch [18/10000], Loss: 765.2355\n",
      "Epoch [19/10000], Loss: 736.8620\n",
      "Epoch [20/10000], Loss: 709.2207\n",
      "Epoch [21/10000], Loss: 682.7543\n",
      "Epoch [22/10000], Loss: 657.9791\n",
      "Epoch [23/10000], Loss: 633.8425\n",
      "Epoch [24/10000], Loss: 609.9400\n",
      "Epoch [25/10000], Loss: 588.1955\n",
      "Epoch [26/10000], Loss: 566.3603\n",
      "Epoch [27/10000], Loss: 545.9485\n",
      "Epoch [28/10000], Loss: 526.5192\n",
      "Epoch [29/10000], Loss: 507.5962\n",
      "Epoch [30/10000], Loss: 489.5813\n",
      "Epoch [31/10000], Loss: 472.8577\n",
      "Epoch [32/10000], Loss: 455.6932\n",
      "Epoch [33/10000], Loss: 439.9868\n",
      "Epoch [34/10000], Loss: 424.2050\n",
      "Epoch [35/10000], Loss: 409.8248\n",
      "Epoch [36/10000], Loss: 395.5406\n",
      "Epoch [37/10000], Loss: 382.6667\n",
      "Epoch [38/10000], Loss: 369.8146\n",
      "Epoch [39/10000], Loss: 358.2856\n",
      "Epoch [40/10000], Loss: 346.6810\n",
      "Epoch [41/10000], Loss: 336.3235\n",
      "Epoch [42/10000], Loss: 325.7483\n",
      "Epoch [43/10000], Loss: 316.3389\n",
      "Epoch [44/10000], Loss: 306.5266\n",
      "Epoch [45/10000], Loss: 298.2997\n",
      "Epoch [46/10000], Loss: 289.7615\n",
      "Epoch [47/10000], Loss: 281.5066\n",
      "Epoch [48/10000], Loss: 274.5294\n",
      "Epoch [49/10000], Loss: 267.4882\n",
      "Epoch [50/10000], Loss: 260.5867\n",
      "Epoch [51/10000], Loss: 254.8229\n",
      "Epoch [52/10000], Loss: 248.4941\n",
      "Epoch [53/10000], Loss: 243.6480\n",
      "Epoch [54/10000], Loss: 238.4447\n",
      "Epoch [55/10000], Loss: 233.1573\n",
      "Epoch [56/10000], Loss: 229.1923\n",
      "Epoch [57/10000], Loss: 224.8938\n",
      "Epoch [58/10000], Loss: 220.4661\n",
      "Epoch [59/10000], Loss: 217.3823\n",
      "Epoch [60/10000], Loss: 213.8127\n",
      "Epoch [61/10000], Loss: 210.4376\n",
      "Epoch [62/10000], Loss: 206.8692\n",
      "Epoch [63/10000], Loss: 204.3700\n",
      "Epoch [64/10000], Loss: 201.4629\n",
      "Epoch [65/10000], Loss: 198.8371\n",
      "Epoch [66/10000], Loss: 195.8292\n",
      "Epoch [67/10000], Loss: 193.9594\n",
      "Epoch [68/10000], Loss: 191.8848\n",
      "Epoch [69/10000], Loss: 189.8870\n",
      "Epoch [70/10000], Loss: 188.1152\n",
      "Epoch [71/10000], Loss: 185.7901\n",
      "Epoch [72/10000], Loss: 184.6663\n",
      "Epoch [73/10000], Loss: 183.1695\n",
      "Epoch [74/10000], Loss: 181.6406\n",
      "Epoch [75/10000], Loss: 180.1812\n",
      "Epoch [76/10000], Loss: 178.2715\n",
      "Epoch [77/10000], Loss: 177.5293\n",
      "Epoch [78/10000], Loss: 176.1743\n",
      "Epoch [79/10000], Loss: 174.8667\n",
      "Epoch [80/10000], Loss: 173.5951\n",
      "Epoch [81/10000], Loss: 172.3353\n",
      "Epoch [82/10000], Loss: 171.1216\n",
      "Epoch [83/10000], Loss: 169.4415\n",
      "Epoch [84/10000], Loss: 168.9620\n",
      "Epoch [85/10000], Loss: 167.8632\n",
      "Epoch [86/10000], Loss: 166.8713\n",
      "Epoch [87/10000], Loss: 165.9035\n",
      "Epoch [88/10000], Loss: 164.8938\n",
      "Epoch [89/10000], Loss: 163.3953\n",
      "Epoch [90/10000], Loss: 163.1940\n",
      "Epoch [91/10000], Loss: 162.3235\n",
      "Epoch [92/10000], Loss: 161.5025\n",
      "Epoch [93/10000], Loss: 160.7878\n",
      "Epoch [94/10000], Loss: 159.8974\n",
      "Epoch [95/10000], Loss: 159.3251\n",
      "Epoch [96/10000], Loss: 158.0963\n",
      "Epoch [97/10000], Loss: 158.0031\n",
      "Epoch [98/10000], Loss: 157.3584\n",
      "Epoch [99/10000], Loss: 156.8447\n",
      "Epoch [100/10000], Loss: 156.2173\n",
      "Epoch [101/10000], Loss: 155.6442\n",
      "Epoch [102/10000], Loss: 154.5368\n",
      "Epoch [103/10000], Loss: 154.6596\n",
      "Epoch [104/10000], Loss: 154.1244\n",
      "Epoch [105/10000], Loss: 153.6100\n",
      "Epoch [106/10000], Loss: 153.1197\n",
      "Epoch [107/10000], Loss: 152.6381\n",
      "Epoch [108/10000], Loss: 152.1567\n",
      "Epoch [109/10000], Loss: 151.1524\n",
      "Epoch [110/10000], Loss: 151.3636\n",
      "Epoch [111/10000], Loss: 150.9168\n",
      "Epoch [112/10000], Loss: 150.4929\n",
      "Epoch [113/10000], Loss: 150.0843\n",
      "Epoch [114/10000], Loss: 149.7012\n",
      "Epoch [115/10000], Loss: 149.3328\n",
      "Epoch [116/10000], Loss: 148.9845\n",
      "Epoch [117/10000], Loss: 148.6491\n",
      "Epoch [118/10000], Loss: 147.7339\n",
      "Epoch [119/10000], Loss: 148.0979\n",
      "Epoch [120/10000], Loss: 147.7724\n",
      "Epoch [121/10000], Loss: 147.4662\n",
      "Epoch [122/10000], Loss: 147.1722\n",
      "Epoch [123/10000], Loss: 146.8692\n",
      "Epoch [124/10000], Loss: 146.5920\n",
      "Epoch [125/10000], Loss: 146.3014\n",
      "Epoch [126/10000], Loss: 146.0362\n",
      "Epoch [127/10000], Loss: 145.7781\n",
      "Epoch [128/10000], Loss: 145.5327\n",
      "Epoch [129/10000], Loss: 145.2932\n",
      "Epoch [130/10000], Loss: 145.0401\n",
      "Epoch [131/10000], Loss: 144.8149\n",
      "Epoch [132/10000], Loss: 144.5971\n",
      "Epoch [133/10000], Loss: 144.3834\n",
      "Epoch [134/10000], Loss: 144.1731\n",
      "Epoch [135/10000], Loss: 143.9649\n",
      "Epoch [136/10000], Loss: 143.7364\n",
      "Epoch [137/10000], Loss: 143.5284\n",
      "Epoch [138/10000], Loss: 143.3052\n",
      "Epoch [139/10000], Loss: 142.4920\n",
      "Epoch [140/10000], Loss: 142.9769\n",
      "Epoch [141/10000], Loss: 142.7526\n",
      "Epoch [142/10000], Loss: 142.5200\n",
      "Epoch [143/10000], Loss: 142.3086\n",
      "Epoch [144/10000], Loss: 142.0899\n",
      "Epoch [145/10000], Loss: 141.8863\n",
      "Epoch [146/10000], Loss: 141.6832\n",
      "Epoch [147/10000], Loss: 141.4800\n",
      "Epoch [148/10000], Loss: 141.2777\n",
      "Epoch [149/10000], Loss: 141.0756\n",
      "Epoch [150/10000], Loss: 140.8736\n",
      "Epoch [151/10000], Loss: 140.6718\n",
      "Epoch [152/10000], Loss: 140.4698\n",
      "Epoch [153/10000], Loss: 140.2676\n",
      "Epoch [154/10000], Loss: 140.0653\n",
      "Epoch [155/10000], Loss: 139.8631\n",
      "Epoch [156/10000], Loss: 139.6528\n",
      "Epoch [157/10000], Loss: 139.4499\n",
      "Epoch [158/10000], Loss: 139.2470\n",
      "Epoch [159/10000], Loss: 139.0440\n",
      "Epoch [160/10000], Loss: 138.8412\n",
      "Epoch [161/10000], Loss: 138.6385\n",
      "Epoch [162/10000], Loss: 138.4359\n",
      "Epoch [163/10000], Loss: 138.2336\n",
      "Epoch [164/10000], Loss: 138.0316\n",
      "Epoch [165/10000], Loss: 137.8298\n",
      "Epoch [166/10000], Loss: 137.6267\n",
      "Epoch [167/10000], Loss: 137.4256\n",
      "Epoch [168/10000], Loss: 137.2244\n",
      "Epoch [169/10000], Loss: 137.0233\n",
      "Epoch [170/10000], Loss: 136.8221\n",
      "Epoch [171/10000], Loss: 136.6205\n",
      "Epoch [172/10000], Loss: 136.4186\n",
      "Epoch [173/10000], Loss: 136.2160\n",
      "Epoch [174/10000], Loss: 136.0129\n",
      "Epoch [175/10000], Loss: 135.8092\n",
      "Epoch [176/10000], Loss: 135.6050\n",
      "Epoch [177/10000], Loss: 135.4004\n",
      "Epoch [178/10000], Loss: 135.1949\n",
      "Epoch [179/10000], Loss: 134.9883\n",
      "Epoch [180/10000], Loss: 134.7812\n",
      "Epoch [181/10000], Loss: 134.5729\n",
      "Epoch [182/10000], Loss: 134.3643\n",
      "Epoch [183/10000], Loss: 134.1550\n",
      "Epoch [184/10000], Loss: 133.9443\n",
      "Epoch [185/10000], Loss: 133.7354\n",
      "Epoch [186/10000], Loss: 133.5228\n",
      "Epoch [187/10000], Loss: 133.3104\n",
      "Epoch [188/10000], Loss: 133.0984\n",
      "Epoch [189/10000], Loss: 132.8845\n",
      "Epoch [190/10000], Loss: 132.6700\n",
      "Epoch [191/10000], Loss: 132.4560\n",
      "Epoch [192/10000], Loss: 132.2413\n",
      "Epoch [193/10000], Loss: 132.0252\n",
      "Epoch [194/10000], Loss: 131.8085\n",
      "Epoch [195/10000], Loss: 131.5920\n",
      "Epoch [196/10000], Loss: 131.3743\n",
      "Epoch [197/10000], Loss: 131.1560\n",
      "Epoch [198/10000], Loss: 130.9370\n",
      "Epoch [199/10000], Loss: 130.7178\n",
      "Epoch [200/10000], Loss: 130.4981\n",
      "Epoch [201/10000], Loss: 130.2986\n",
      "Epoch [202/10000], Loss: 130.0690\n",
      "Epoch [203/10000], Loss: 129.8421\n",
      "Epoch [204/10000], Loss: 129.6236\n",
      "Epoch [205/10000], Loss: 129.4015\n",
      "Epoch [206/10000], Loss: 129.1790\n",
      "Epoch [207/10000], Loss: 128.9551\n",
      "Epoch [208/10000], Loss: 128.7303\n",
      "Epoch [209/10000], Loss: 128.5046\n",
      "Epoch [210/10000], Loss: 128.2775\n",
      "Epoch [211/10000], Loss: 128.0501\n",
      "Epoch [212/10000], Loss: 127.8222\n",
      "Epoch [213/10000], Loss: 127.5936\n",
      "Epoch [214/10000], Loss: 127.3644\n",
      "Epoch [215/10000], Loss: 127.1348\n",
      "Epoch [216/10000], Loss: 126.9044\n",
      "Epoch [217/10000], Loss: 126.6735\n",
      "Epoch [218/10000], Loss: 126.4418\n",
      "Epoch [219/10000], Loss: 126.2096\n",
      "Epoch [220/10000], Loss: 125.9768\n",
      "Epoch [221/10000], Loss: 125.7432\n",
      "Epoch [222/10000], Loss: 125.5090\n",
      "Epoch [223/10000], Loss: 125.2741\n",
      "Epoch [224/10000], Loss: 125.0387\n",
      "Epoch [225/10000], Loss: 124.8026\n",
      "Epoch [226/10000], Loss: 124.5659\n",
      "Epoch [227/10000], Loss: 124.3285\n",
      "Epoch [228/10000], Loss: 124.0904\n",
      "Epoch [229/10000], Loss: 123.8517\n",
      "Epoch [230/10000], Loss: 123.6122\n",
      "Epoch [231/10000], Loss: 123.3722\n",
      "Epoch [232/10000], Loss: 123.1314\n",
      "Epoch [233/10000], Loss: 122.8901\n",
      "Epoch [234/10000], Loss: 122.6479\n",
      "Epoch [235/10000], Loss: 122.4051\n",
      "Epoch [236/10000], Loss: 122.1611\n",
      "Epoch [237/10000], Loss: 121.9168\n",
      "Epoch [238/10000], Loss: 121.6719\n",
      "Epoch [239/10000], Loss: 121.4262\n",
      "Epoch [240/10000], Loss: 121.1800\n",
      "Epoch [241/10000], Loss: 120.9332\n",
      "Epoch [242/10000], Loss: 120.6856\n",
      "Epoch [243/10000], Loss: 120.4374\n",
      "Epoch [244/10000], Loss: 120.1886\n",
      "Epoch [245/10000], Loss: 119.9391\n",
      "Epoch [246/10000], Loss: 119.6888\n",
      "Epoch [247/10000], Loss: 119.4379\n",
      "Epoch [248/10000], Loss: 119.1863\n",
      "Epoch [249/10000], Loss: 118.9340\n",
      "Epoch [250/10000], Loss: 118.6807\n",
      "Epoch [251/10000], Loss: 118.4506\n",
      "Epoch [252/10000], Loss: 118.1831\n",
      "Epoch [253/10000], Loss: 117.9371\n",
      "Epoch [254/10000], Loss: 117.6876\n",
      "Epoch [255/10000], Loss: 117.4351\n",
      "Epoch [256/10000], Loss: 117.1795\n",
      "Epoch [257/10000], Loss: 116.9357\n",
      "Epoch [258/10000], Loss: 116.6900\n",
      "Epoch [259/10000], Loss: 116.4364\n",
      "Epoch [260/10000], Loss: 116.1821\n",
      "Epoch [261/10000], Loss: 115.9474\n",
      "Epoch [262/10000], Loss: 115.6836\n",
      "Epoch [263/10000], Loss: 115.4227\n",
      "Epoch [264/10000], Loss: 115.1614\n",
      "Epoch [265/10000], Loss: 114.8972\n",
      "Epoch [266/10000], Loss: 114.6312\n",
      "Epoch [267/10000], Loss: 114.3640\n",
      "Epoch [268/10000], Loss: 114.0962\n",
      "Epoch [269/10000], Loss: 113.8276\n",
      "Epoch [270/10000], Loss: 113.5585\n",
      "Epoch [271/10000], Loss: 113.2888\n",
      "Epoch [272/10000], Loss: 113.0185\n",
      "Epoch [273/10000], Loss: 112.7476\n",
      "Epoch [274/10000], Loss: 112.4762\n",
      "Epoch [275/10000], Loss: 112.2043\n",
      "Epoch [276/10000], Loss: 111.9318\n",
      "Epoch [277/10000], Loss: 111.6584\n",
      "Epoch [278/10000], Loss: 111.3861\n",
      "Epoch [279/10000], Loss: 111.1105\n",
      "Epoch [280/10000], Loss: 110.8355\n",
      "Epoch [281/10000], Loss: 110.5601\n",
      "Epoch [282/10000], Loss: 110.2835\n",
      "Epoch [283/10000], Loss: 110.0070\n",
      "Epoch [284/10000], Loss: 109.7287\n",
      "Epoch [285/10000], Loss: 109.4496\n",
      "Epoch [286/10000], Loss: 109.1702\n",
      "Epoch [287/10000], Loss: 108.8897\n",
      "Epoch [288/10000], Loss: 108.6085\n",
      "Epoch [289/10000], Loss: 108.3268\n",
      "Epoch [290/10000], Loss: 108.0444\n",
      "Epoch [291/10000], Loss: 107.7614\n",
      "Epoch [292/10000], Loss: 107.4773\n",
      "Epoch [293/10000], Loss: 107.1928\n",
      "Epoch [294/10000], Loss: 106.9076\n",
      "Epoch [295/10000], Loss: 106.6216\n",
      "Epoch [296/10000], Loss: 106.3349\n",
      "Epoch [297/10000], Loss: 106.0474\n",
      "Epoch [298/10000], Loss: 105.7591\n",
      "Epoch [299/10000], Loss: 105.4702\n",
      "Epoch [300/10000], Loss: 105.1804\n",
      "Epoch [301/10000], Loss: 104.8898\n",
      "Epoch [302/10000], Loss: 104.5984\n",
      "Epoch [303/10000], Loss: 104.3344\n",
      "Epoch [304/10000], Loss: 104.0368\n",
      "Epoch [305/10000], Loss: 103.8147\n",
      "Epoch [306/10000], Loss: 103.5049\n",
      "Epoch [307/10000], Loss: 103.2279\n",
      "Epoch [308/10000], Loss: 102.9555\n",
      "Epoch [309/10000], Loss: 102.6597\n",
      "Epoch [310/10000], Loss: 102.3638\n",
      "Epoch [311/10000], Loss: 102.0634\n",
      "Epoch [312/10000], Loss: 101.7606\n",
      "Epoch [313/10000], Loss: 101.4564\n",
      "Epoch [314/10000], Loss: 101.1514\n",
      "Epoch [315/10000], Loss: 100.8456\n",
      "Epoch [316/10000], Loss: 100.5393\n",
      "Epoch [317/10000], Loss: 100.2326\n",
      "Epoch [318/10000], Loss: 99.9253\n",
      "Epoch [319/10000], Loss: 99.6175\n",
      "Epoch [320/10000], Loss: 99.3092\n",
      "Epoch [321/10000], Loss: 98.9999\n",
      "Epoch [322/10000], Loss: 98.6904\n",
      "Epoch [323/10000], Loss: 98.3806\n",
      "Epoch [324/10000], Loss: 98.0700\n",
      "Epoch [325/10000], Loss: 97.7590\n",
      "Epoch [326/10000], Loss: 97.4465\n",
      "Epoch [327/10000], Loss: 97.1342\n",
      "Epoch [328/10000], Loss: 96.8218\n",
      "Epoch [329/10000], Loss: 96.5083\n",
      "Epoch [330/10000], Loss: 96.1936\n",
      "Epoch [331/10000], Loss: 95.8792\n",
      "Epoch [332/10000], Loss: 95.5637\n",
      "Epoch [333/10000], Loss: 95.2480\n",
      "Epoch [334/10000], Loss: 94.9312\n",
      "Epoch [335/10000], Loss: 94.6137\n",
      "Epoch [336/10000], Loss: 94.2956\n",
      "Epoch [337/10000], Loss: 93.9771\n",
      "Epoch [338/10000], Loss: 93.6574\n",
      "Epoch [339/10000], Loss: 93.3373\n",
      "Epoch [340/10000], Loss: 93.0161\n",
      "Epoch [341/10000], Loss: 92.6945\n",
      "Epoch [342/10000], Loss: 92.3721\n",
      "Epoch [343/10000], Loss: 92.0490\n",
      "Epoch [344/10000], Loss: 91.7252\n",
      "Epoch [345/10000], Loss: 91.4006\n",
      "Epoch [346/10000], Loss: 91.0752\n",
      "Epoch [347/10000], Loss: 90.7584\n",
      "Epoch [348/10000], Loss: 90.6102\n",
      "Epoch [349/10000], Loss: 90.1672\n",
      "Epoch [350/10000], Loss: 89.9100\n",
      "Epoch [351/10000], Loss: 89.5594\n",
      "Epoch [352/10000], Loss: 89.2333\n",
      "Epoch [353/10000], Loss: 88.9037\n",
      "Epoch [354/10000], Loss: 88.5706\n",
      "Epoch [355/10000], Loss: 88.2353\n",
      "Epoch [356/10000], Loss: 87.8989\n",
      "Epoch [357/10000], Loss: 87.5618\n",
      "Epoch [358/10000], Loss: 87.2242\n",
      "Epoch [359/10000], Loss: 86.8862\n",
      "Epoch [360/10000], Loss: 86.5476\n",
      "Epoch [361/10000], Loss: 86.2085\n",
      "Epoch [362/10000], Loss: 85.8691\n",
      "Epoch [363/10000], Loss: 85.5292\n",
      "Epoch [364/10000], Loss: 85.1888\n",
      "Epoch [365/10000], Loss: 84.8479\n",
      "Epoch [366/10000], Loss: 84.5066\n",
      "Epoch [367/10000], Loss: 84.1646\n",
      "Epoch [368/10000], Loss: 83.8223\n",
      "Epoch [369/10000], Loss: 83.4794\n",
      "Epoch [370/10000], Loss: 83.1360\n",
      "Epoch [371/10000], Loss: 82.7921\n",
      "Epoch [372/10000], Loss: 82.4475\n",
      "Epoch [373/10000], Loss: 82.1026\n",
      "Epoch [374/10000], Loss: 81.7571\n",
      "Epoch [375/10000], Loss: 81.4110\n",
      "Epoch [376/10000], Loss: 81.0632\n",
      "Epoch [377/10000], Loss: 80.7167\n",
      "Epoch [378/10000], Loss: 80.3694\n",
      "Epoch [379/10000], Loss: 80.0203\n",
      "Epoch [380/10000], Loss: 79.6796\n",
      "Epoch [381/10000], Loss: 79.3529\n",
      "Epoch [382/10000], Loss: 79.1015\n",
      "Epoch [383/10000], Loss: 78.7089\n",
      "Epoch [384/10000], Loss: 78.3956\n",
      "Epoch [385/10000], Loss: 78.0479\n",
      "Epoch [386/10000], Loss: 77.6995\n",
      "Epoch [387/10000], Loss: 77.3468\n",
      "Epoch [388/10000], Loss: 76.9908\n",
      "Epoch [389/10000], Loss: 76.6334\n",
      "Epoch [390/10000], Loss: 76.2755\n",
      "Epoch [391/10000], Loss: 75.9171\n",
      "Epoch [392/10000], Loss: 75.5586\n",
      "Epoch [393/10000], Loss: 75.1999\n",
      "Epoch [394/10000], Loss: 74.8410\n",
      "Epoch [395/10000], Loss: 74.4819\n",
      "Epoch [396/10000], Loss: 74.1226\n",
      "Epoch [397/10000], Loss: 73.7632\n",
      "Epoch [398/10000], Loss: 73.4036\n",
      "Epoch [399/10000], Loss: 73.0438\n",
      "Epoch [400/10000], Loss: 72.6838\n",
      "Epoch [401/10000], Loss: 72.3236\n",
      "Epoch [402/10000], Loss: 71.9632\n",
      "Epoch [403/10000], Loss: 71.6026\n",
      "Epoch [404/10000], Loss: 71.2419\n",
      "Epoch [405/10000], Loss: 70.8809\n",
      "Epoch [406/10000], Loss: 70.5199\n",
      "Epoch [407/10000], Loss: 70.1586\n",
      "Epoch [408/10000], Loss: 69.7963\n",
      "Epoch [409/10000], Loss: 69.4351\n",
      "Epoch [410/10000], Loss: 69.0782\n",
      "Epoch [411/10000], Loss: 68.7465\n",
      "Epoch [412/10000], Loss: 68.4700\n",
      "Epoch [413/10000], Loss: 68.0767\n",
      "Epoch [414/10000], Loss: 67.7217\n",
      "Epoch [415/10000], Loss: 67.4033\n",
      "Epoch [416/10000], Loss: 67.0226\n",
      "Epoch [417/10000], Loss: 66.6608\n",
      "Epoch [418/10000], Loss: 66.2956\n",
      "Epoch [419/10000], Loss: 65.9289\n",
      "Epoch [420/10000], Loss: 65.5619\n",
      "Epoch [421/10000], Loss: 65.1950\n",
      "Epoch [422/10000], Loss: 64.8284\n",
      "Epoch [423/10000], Loss: 64.4620\n",
      "Epoch [424/10000], Loss: 64.0959\n",
      "Epoch [425/10000], Loss: 63.7301\n",
      "Epoch [426/10000], Loss: 63.3646\n",
      "Epoch [427/10000], Loss: 62.9993\n",
      "Epoch [428/10000], Loss: 62.6344\n",
      "Epoch [429/10000], Loss: 62.2698\n",
      "Epoch [430/10000], Loss: 61.9055\n",
      "Epoch [431/10000], Loss: 61.5416\n",
      "Epoch [432/10000], Loss: 61.1780\n",
      "Epoch [433/10000], Loss: 60.8147\n",
      "Epoch [434/10000], Loss: 60.4518\n",
      "Epoch [435/10000], Loss: 60.0893\n",
      "Epoch [436/10000], Loss: 59.7272\n",
      "Epoch [437/10000], Loss: 59.3655\n",
      "Epoch [438/10000], Loss: 59.0162\n",
      "Epoch [439/10000], Loss: 58.6832\n",
      "Epoch [440/10000], Loss: 58.3895\n",
      "Epoch [441/10000], Loss: 58.0127\n",
      "Epoch [442/10000], Loss: 57.6555\n",
      "Epoch [443/10000], Loss: 57.2939\n",
      "Epoch [444/10000], Loss: 56.9727\n",
      "Epoch [445/10000], Loss: 56.5986\n",
      "Epoch [446/10000], Loss: 56.2402\n",
      "Epoch [447/10000], Loss: 55.8800\n",
      "Epoch [448/10000], Loss: 55.5202\n",
      "Epoch [449/10000], Loss: 55.1614\n",
      "Epoch [450/10000], Loss: 54.8033\n",
      "Epoch [451/10000], Loss: 54.4463\n",
      "Epoch [452/10000], Loss: 54.0903\n",
      "Epoch [453/10000], Loss: 53.7353\n",
      "Epoch [454/10000], Loss: 53.3813\n",
      "Epoch [455/10000], Loss: 53.0283\n",
      "Epoch [456/10000], Loss: 52.6765\n",
      "Epoch [457/10000], Loss: 52.3257\n",
      "Epoch [458/10000], Loss: 51.9761\n",
      "Epoch [459/10000], Loss: 51.6275\n",
      "Epoch [460/10000], Loss: 51.2802\n",
      "Epoch [461/10000], Loss: 50.9340\n",
      "Epoch [462/10000], Loss: 50.5891\n",
      "Epoch [463/10000], Loss: 50.2454\n",
      "Epoch [464/10000], Loss: 49.9029\n",
      "Epoch [465/10000], Loss: 49.5932\n",
      "Epoch [466/10000], Loss: 49.2718\n",
      "Epoch [467/10000], Loss: 48.9889\n",
      "Epoch [468/10000], Loss: 48.6372\n",
      "Epoch [469/10000], Loss: 48.2941\n",
      "Epoch [470/10000], Loss: 47.9539\n",
      "Epoch [471/10000], Loss: 47.6198\n",
      "Epoch [472/10000], Loss: 47.3150\n",
      "Epoch [473/10000], Loss: 46.9791\n",
      "Epoch [474/10000], Loss: 46.6455\n",
      "Epoch [475/10000], Loss: 46.3150\n",
      "Epoch [476/10000], Loss: 45.9867\n",
      "Epoch [477/10000], Loss: 45.6604\n",
      "Epoch [478/10000], Loss: 45.3361\n",
      "Epoch [479/10000], Loss: 45.0138\n",
      "Epoch [480/10000], Loss: 44.6935\n",
      "Epoch [481/10000], Loss: 44.3752\n",
      "Epoch [482/10000], Loss: 44.0590\n",
      "Epoch [483/10000], Loss: 43.7450\n",
      "Epoch [484/10000], Loss: 43.4330\n",
      "Epoch [485/10000], Loss: 43.1233\n",
      "Epoch [486/10000], Loss: 42.8158\n",
      "Epoch [487/10000], Loss: 42.5105\n",
      "Epoch [488/10000], Loss: 42.2075\n",
      "Epoch [489/10000], Loss: 41.9068\n",
      "Epoch [490/10000], Loss: 41.6232\n",
      "Epoch [491/10000], Loss: 41.3755\n",
      "Epoch [492/10000], Loss: 41.0921\n",
      "Epoch [493/10000], Loss: 40.8221\n",
      "Epoch [494/10000], Loss: 40.5112\n",
      "Epoch [495/10000], Loss: 40.2166\n",
      "Epoch [496/10000], Loss: 39.9292\n",
      "Epoch [497/10000], Loss: 39.6452\n",
      "Epoch [498/10000], Loss: 39.3990\n",
      "Epoch [499/10000], Loss: 39.1215\n",
      "Epoch [500/10000], Loss: 38.8341\n",
      "Epoch [501/10000], Loss: 38.5580\n",
      "Epoch [502/10000], Loss: 38.2872\n",
      "Epoch [503/10000], Loss: 38.0198\n",
      "Epoch [504/10000], Loss: 37.7555\n",
      "Epoch [505/10000], Loss: 37.4939\n",
      "Epoch [506/10000], Loss: 37.2359\n",
      "Epoch [507/10000], Loss: 36.9810\n",
      "Epoch [508/10000], Loss: 36.7292\n",
      "Epoch [509/10000], Loss: 36.4806\n",
      "Epoch [510/10000], Loss: 36.2351\n",
      "Epoch [511/10000], Loss: 35.9928\n",
      "Epoch [512/10000], Loss: 35.7538\n",
      "Epoch [513/10000], Loss: 35.5180\n",
      "Epoch [514/10000], Loss: 35.2948\n",
      "Epoch [515/10000], Loss: 35.1345\n",
      "Epoch [516/10000], Loss: 34.8841\n",
      "Epoch [517/10000], Loss: 34.7045\n",
      "Epoch [518/10000], Loss: 34.4449\n",
      "Epoch [519/10000], Loss: 34.2201\n",
      "Epoch [520/10000], Loss: 34.0047\n",
      "Epoch [521/10000], Loss: 33.7934\n",
      "Epoch [522/10000], Loss: 33.5911\n",
      "Epoch [523/10000], Loss: 33.4251\n",
      "Epoch [524/10000], Loss: 33.2006\n",
      "Epoch [525/10000], Loss: 32.9994\n",
      "Epoch [526/10000], Loss: 32.8053\n",
      "Epoch [527/10000], Loss: 32.6152\n",
      "Epoch [528/10000], Loss: 32.0253\n",
      "Epoch [529/10000], Loss: 32.1415\n",
      "Epoch [530/10000], Loss: 32.0930\n",
      "Epoch [531/10000], Loss: 31.9272\n",
      "Epoch [532/10000], Loss: 31.7528\n",
      "Epoch [533/10000], Loss: 31.5816\n",
      "Epoch [534/10000], Loss: 31.4145\n",
      "Epoch [535/10000], Loss: 31.2515\n",
      "Epoch [536/10000], Loss: 31.0926\n",
      "Epoch [537/10000], Loss: 30.9606\n",
      "Epoch [538/10000], Loss: 30.8678\n",
      "Epoch [539/10000], Loss: 30.6901\n",
      "Epoch [540/10000], Loss: 30.5836\n",
      "Epoch [541/10000], Loss: 30.3942\n",
      "Epoch [542/10000], Loss: 30.2554\n",
      "Epoch [543/10000], Loss: 30.2082\n",
      "Epoch [544/10000], Loss: 30.0040\n",
      "Epoch [545/10000], Loss: 29.8715\n",
      "Epoch [546/10000], Loss: 29.7928\n",
      "Epoch [547/10000], Loss: 29.6425\n",
      "Epoch [548/10000], Loss: 29.5242\n",
      "Epoch [549/10000], Loss: 29.4125\n",
      "Epoch [550/10000], Loss: 29.3046\n",
      "Epoch [551/10000], Loss: 29.2002\n",
      "Epoch [552/10000], Loss: 29.0998\n",
      "Epoch [553/10000], Loss: 29.0027\n",
      "Epoch [554/10000], Loss: 28.9097\n",
      "Epoch [555/10000], Loss: 28.9099\n",
      "Epoch [556/10000], Loss: 28.7437\n",
      "Epoch [557/10000], Loss: 28.6536\n",
      "Epoch [558/10000], Loss: 28.5731\n",
      "Epoch [559/10000], Loss: 28.5064\n",
      "Epoch [560/10000], Loss: 28.4963\n",
      "Epoch [561/10000], Loss: 28.3757\n",
      "Epoch [562/10000], Loss: 28.3559\n",
      "Epoch [563/10000], Loss: 28.2399\n",
      "Epoch [564/10000], Loss: 28.1734\n",
      "Epoch [565/10000], Loss: 28.1135\n",
      "Epoch [566/10000], Loss: 28.0591\n",
      "Epoch [567/10000], Loss: 28.0854\n",
      "Epoch [568/10000], Loss: 27.9596\n",
      "Epoch [569/10000], Loss: 27.9424\n",
      "Epoch [570/10000], Loss: 27.8663\n",
      "Epoch [571/10000], Loss: 27.8207\n",
      "Epoch [572/10000], Loss: 27.7797\n",
      "Epoch [573/10000], Loss: 27.7426\n",
      "Epoch [574/10000], Loss: 27.7606\n",
      "Epoch [575/10000], Loss: 27.6755\n",
      "Epoch [576/10000], Loss: 27.6413\n",
      "Epoch [577/10000], Loss: 27.6851\n",
      "Epoch [578/10000], Loss: 27.5894\n",
      "Epoch [579/10000], Loss: 27.5605\n",
      "Epoch [580/10000], Loss: 27.5376\n",
      "Epoch [581/10000], Loss: 27.5169\n",
      "Epoch [582/10000], Loss: 27.5052\n",
      "Epoch [583/10000], Loss: 27.5755\n",
      "Epoch [584/10000], Loss: 27.4792\n",
      "Epoch [585/10000], Loss: 27.4947\n",
      "Epoch [586/10000], Loss: 27.4488\n",
      "Epoch [587/10000], Loss: 27.4922\n",
      "Epoch [588/10000], Loss: 27.4266\n",
      "Epoch [589/10000], Loss: 27.4154\n",
      "Epoch [590/10000], Loss: 27.4086\n",
      "Epoch [591/10000], Loss: 27.4046\n",
      "Epoch [592/10000], Loss: 27.4380\n",
      "Epoch [593/10000], Loss: 27.4175\n",
      "Epoch [594/10000], Loss: 27.3974\n",
      "Epoch [595/10000], Loss: 27.4426\n",
      "Epoch [596/10000], Loss: 27.3972\n",
      "Epoch [597/10000], Loss: 27.3955\n",
      "Epoch [598/10000], Loss: 27.3978\n",
      "Epoch [599/10000], Loss: 27.4303\n",
      "Epoch [600/10000], Loss: 27.4036\n",
      "Epoch [601/10000], Loss: 27.4058\n",
      "Epoch [602/10000], Loss: 27.4488\n",
      "Epoch [603/10000], Loss: 27.3540\n",
      "Epoch [604/10000], Loss: 27.1798\n",
      "Epoch [605/10000], Loss: 27.4507\n",
      "Epoch [606/10000], Loss: 27.4909\n",
      "Epoch [607/10000], Loss: 27.4733\n",
      "Epoch [608/10000], Loss: 27.5225\n",
      "Epoch [609/10000], Loss: 27.4850\n",
      "Epoch [610/10000], Loss: 27.5073\n",
      "Epoch [611/10000], Loss: 27.4963\n",
      "Epoch [612/10000], Loss: 27.5035\n",
      "Epoch [613/10000], Loss: 27.5126\n",
      "Epoch [614/10000], Loss: 27.5411\n",
      "Epoch [615/10000], Loss: 27.5437\n",
      "Epoch [616/10000], Loss: 27.5403\n",
      "Epoch [617/10000], Loss: 27.5523\n",
      "Epoch [618/10000], Loss: 27.5871\n",
      "Epoch [619/10000], Loss: 27.5702\n",
      "Epoch [620/10000], Loss: 27.5797\n",
      "Epoch [621/10000], Loss: 27.6124\n",
      "Epoch [622/10000], Loss: 27.6020\n",
      "Epoch [623/10000], Loss: 27.6122\n",
      "Epoch [624/10000], Loss: 27.6383\n",
      "Epoch [625/10000], Loss: 27.6354\n",
      "Epoch [626/10000], Loss: 27.6572\n",
      "Epoch [627/10000], Loss: 27.6579\n",
      "Epoch [628/10000], Loss: 27.6748\n",
      "Epoch [629/10000], Loss: 27.6850\n",
      "Epoch [630/10000], Loss: 27.6941\n",
      "Epoch [631/10000], Loss: 27.7024\n",
      "Epoch [632/10000], Loss: 27.7101\n",
      "Epoch [633/10000], Loss: 27.7170\n",
      "Epoch [634/10000], Loss: 27.7233\n",
      "Epoch [635/10000], Loss: 27.7288\n",
      "Epoch [636/10000], Loss: 27.7297\n",
      "Epoch [637/10000], Loss: 27.6641\n",
      "Epoch [638/10000], Loss: 27.6438\n",
      "Epoch [639/10000], Loss: 27.6242\n",
      "Epoch [640/10000], Loss: 27.6074\n",
      "Epoch [641/10000], Loss: 27.5913\n",
      "Epoch [642/10000], Loss: 27.5758\n",
      "Epoch [643/10000], Loss: 27.5606\n",
      "Epoch [644/10000], Loss: 27.5458\n",
      "Epoch [645/10000], Loss: 27.5314\n",
      "Epoch [646/10000], Loss: 27.5172\n",
      "Epoch [647/10000], Loss: 27.5032\n",
      "Epoch [648/10000], Loss: 27.4895\n",
      "Epoch [649/10000], Loss: 27.4767\n",
      "Epoch [650/10000], Loss: 27.4632\n",
      "Epoch [651/10000], Loss: 27.4491\n",
      "Epoch [652/10000], Loss: 27.4361\n",
      "Epoch [653/10000], Loss: 27.4225\n",
      "Epoch [654/10000], Loss: 27.4098\n",
      "Epoch [655/10000], Loss: 27.3964\n",
      "Epoch [656/10000], Loss: 27.3840\n",
      "Epoch [657/10000], Loss: 27.3705\n",
      "Epoch [658/10000], Loss: 27.3585\n",
      "Epoch [659/10000], Loss: 27.3447\n",
      "Epoch [660/10000], Loss: 27.3337\n",
      "Epoch [661/10000], Loss: 27.3183\n",
      "Epoch [662/10000], Loss: 27.3105\n",
      "Epoch [663/10000], Loss: 27.2892\n",
      "Epoch [664/10000], Loss: 27.2934\n",
      "Epoch [665/10000], Loss: 27.2482\n",
      "Epoch [666/10000], Loss: 27.3056\n",
      "Epoch [667/10000], Loss: 27.1715\n",
      "Epoch [668/10000], Loss: 27.3398\n",
      "Epoch [669/10000], Loss: 27.1559\n",
      "Epoch [670/10000], Loss: 27.2679\n",
      "Epoch [671/10000], Loss: 27.1696\n",
      "Epoch [672/10000], Loss: 27.2154\n",
      "Epoch [673/10000], Loss: 27.1597\n",
      "Epoch [674/10000], Loss: 27.1803\n",
      "Epoch [675/10000], Loss: 27.1396\n",
      "Epoch [676/10000], Loss: 27.1518\n",
      "Epoch [677/10000], Loss: 27.1150\n",
      "Epoch [678/10000], Loss: 27.1272\n",
      "Epoch [679/10000], Loss: 27.0872\n",
      "Epoch [680/10000], Loss: 27.1064\n",
      "Epoch [681/10000], Loss: 27.0549\n",
      "Epoch [682/10000], Loss: 27.0929\n",
      "Epoch [683/10000], Loss: 27.0147\n",
      "Epoch [684/10000], Loss: 27.0832\n",
      "Epoch [685/10000], Loss: 27.3739\n",
      "Epoch [686/10000], Loss: 27.3385\n",
      "Epoch [687/10000], Loss: 27.3197\n",
      "Epoch [688/10000], Loss: 27.3018\n",
      "Epoch [689/10000], Loss: 27.2842\n",
      "Epoch [690/10000], Loss: 27.2667\n",
      "Epoch [691/10000], Loss: 27.2494\n",
      "Epoch [692/10000], Loss: 27.2320\n",
      "Epoch [693/10000], Loss: 27.2147\n",
      "Epoch [694/10000], Loss: 27.1974\n",
      "Epoch [695/10000], Loss: 27.1801\n",
      "Epoch [696/10000], Loss: 27.1628\n",
      "Epoch [697/10000], Loss: 27.1455\n",
      "Epoch [698/10000], Loss: 27.1282\n",
      "Epoch [699/10000], Loss: 27.1109\n",
      "Epoch [700/10000], Loss: 27.0934\n",
      "Epoch [701/10000], Loss: 27.0760\n",
      "Epoch [702/10000], Loss: 27.0586\n",
      "Epoch [703/10000], Loss: 27.0412\n",
      "Epoch [704/10000], Loss: 27.0238\n",
      "Epoch [705/10000], Loss: 26.9080\n",
      "Epoch [706/10000], Loss: 27.0359\n",
      "Epoch [707/10000], Loss: 27.0260\n",
      "Epoch [708/10000], Loss: 26.9249\n",
      "Epoch [709/10000], Loss: 26.8612\n",
      "Epoch [710/10000], Loss: 26.8203\n",
      "Epoch [711/10000], Loss: 26.7922\n",
      "Epoch [712/10000], Loss: 26.7716\n",
      "Epoch [713/10000], Loss: 26.7550\n",
      "Epoch [714/10000], Loss: 26.7405\n",
      "Epoch [715/10000], Loss: 26.7271\n",
      "Epoch [716/10000], Loss: 26.7140\n",
      "Epoch [717/10000], Loss: 26.7012\n",
      "Epoch [718/10000], Loss: 26.6884\n",
      "Epoch [719/10000], Loss: 26.6756\n",
      "Epoch [720/10000], Loss: 26.6627\n",
      "Epoch [721/10000], Loss: 26.6500\n",
      "Epoch [722/10000], Loss: 26.6370\n",
      "Epoch [723/10000], Loss: 26.6241\n",
      "Epoch [724/10000], Loss: 26.6112\n",
      "Epoch [725/10000], Loss: 26.5982\n",
      "Epoch [726/10000], Loss: 26.5852\n",
      "Epoch [727/10000], Loss: 26.5722\n",
      "Epoch [728/10000], Loss: 26.5591\n",
      "Epoch [729/10000], Loss: 26.5461\n",
      "Epoch [730/10000], Loss: 26.5331\n",
      "Epoch [731/10000], Loss: 26.5201\n",
      "Epoch [732/10000], Loss: 26.5070\n",
      "Epoch [733/10000], Loss: 26.4940\n",
      "Epoch [734/10000], Loss: 26.4808\n",
      "Epoch [735/10000], Loss: 26.3644\n",
      "Epoch [736/10000], Loss: 26.8324\n",
      "Epoch [737/10000], Loss: 26.7510\n",
      "Epoch [738/10000], Loss: 26.6911\n",
      "Epoch [739/10000], Loss: 26.6399\n",
      "Epoch [740/10000], Loss: 26.5956\n",
      "Epoch [741/10000], Loss: 26.5569\n",
      "Epoch [742/10000], Loss: 26.4110\n",
      "Epoch [743/10000], Loss: 26.9213\n",
      "Epoch [744/10000], Loss: 26.8414\n",
      "Epoch [745/10000], Loss: 26.7839\n",
      "Epoch [746/10000], Loss: 26.7318\n",
      "Epoch [747/10000], Loss: 26.6842\n",
      "Epoch [748/10000], Loss: 26.6403\n",
      "Epoch [749/10000], Loss: 26.5999\n",
      "Epoch [750/10000], Loss: 26.5626\n",
      "Epoch [751/10000], Loss: 26.5280\n",
      "Epoch [752/10000], Loss: 26.4960\n",
      "Epoch [753/10000], Loss: 26.4661\n",
      "Epoch [754/10000], Loss: 26.4382\n",
      "Epoch [755/10000], Loss: 26.4122\n",
      "Epoch [756/10000], Loss: 26.3876\n",
      "Epoch [757/10000], Loss: 26.2854\n",
      "Epoch [758/10000], Loss: 26.7200\n",
      "Epoch [759/10000], Loss: 26.6387\n",
      "Epoch [760/10000], Loss: 26.5772\n",
      "Epoch [761/10000], Loss: 26.5226\n",
      "Epoch [762/10000], Loss: 26.3887\n",
      "Epoch [763/10000], Loss: 26.8552\n",
      "Epoch [764/10000], Loss: 26.7590\n",
      "Epoch [765/10000], Loss: 26.6875\n",
      "Epoch [766/10000], Loss: 26.6232\n",
      "Epoch [767/10000], Loss: 26.5647\n",
      "Epoch [768/10000], Loss: 26.5113\n",
      "Epoch [769/10000], Loss: 26.4626\n",
      "Epoch [770/10000], Loss: 26.4180\n",
      "Epoch [771/10000], Loss: 26.3770\n",
      "Epoch [772/10000], Loss: 26.3393\n",
      "Epoch [773/10000], Loss: 26.3047\n",
      "Epoch [774/10000], Loss: 26.1818\n",
      "Epoch [775/10000], Loss: 26.6170\n",
      "Epoch [776/10000], Loss: 26.5313\n",
      "Epoch [777/10000], Loss: 26.4670\n",
      "Epoch [778/10000], Loss: 26.4095\n",
      "Epoch [779/10000], Loss: 26.3156\n",
      "Epoch [780/10000], Loss: 26.7285\n",
      "Epoch [781/10000], Loss: 26.6300\n",
      "Epoch [782/10000], Loss: 26.5552\n",
      "Epoch [783/10000], Loss: 26.4898\n",
      "Epoch [784/10000], Loss: 26.4285\n",
      "Epoch [785/10000], Loss: 26.3728\n",
      "Epoch [786/10000], Loss: 26.3221\n",
      "Epoch [787/10000], Loss: 26.2757\n",
      "Epoch [788/10000], Loss: 26.2333\n",
      "Epoch [789/10000], Loss: 26.1942\n",
      "Epoch [790/10000], Loss: 26.0810\n",
      "Epoch [791/10000], Loss: 26.4964\n",
      "Epoch [792/10000], Loss: 26.4097\n",
      "Epoch [793/10000], Loss: 26.3441\n",
      "Epoch [794/10000], Loss: 26.1696\n",
      "Epoch [795/10000], Loss: 26.6515\n",
      "Epoch [796/10000], Loss: 26.5443\n",
      "Epoch [797/10000], Loss: 26.4651\n",
      "Epoch [798/10000], Loss: 26.3937\n",
      "Epoch [799/10000], Loss: 26.3288\n",
      "Epoch [800/10000], Loss: 26.2696\n",
      "Epoch [801/10000], Loss: 26.2157\n",
      "Epoch [802/10000], Loss: 26.1664\n",
      "Epoch [803/10000], Loss: 26.1213\n",
      "Epoch [804/10000], Loss: 26.0799\n",
      "Epoch [805/10000], Loss: 25.9600\n",
      "Epoch [806/10000], Loss: 26.3753\n",
      "Epoch [807/10000], Loss: 26.2873\n",
      "Epoch [808/10000], Loss: 26.2207\n",
      "Epoch [809/10000], Loss: 26.0487\n",
      "Epoch [810/10000], Loss: 26.5247\n",
      "Epoch [811/10000], Loss: 26.4173\n",
      "Epoch [812/10000], Loss: 26.3376\n",
      "Epoch [813/10000], Loss: 26.2658\n",
      "Epoch [814/10000], Loss: 26.2004\n",
      "Epoch [815/10000], Loss: 26.1408\n",
      "Epoch [816/10000], Loss: 26.0863\n",
      "Epoch [817/10000], Loss: 26.0366\n",
      "Epoch [818/10000], Loss: 25.9910\n",
      "Epoch [819/10000], Loss: 25.9491\n",
      "Epoch [820/10000], Loss: 25.8484\n",
      "Epoch [821/10000], Loss: 26.2416\n",
      "Epoch [822/10000], Loss: 26.1542\n",
      "Epoch [823/10000], Loss: 26.0874\n",
      "Epoch [824/10000], Loss: 25.9366\n",
      "Epoch [825/10000], Loss: 26.3889\n",
      "Epoch [826/10000], Loss: 26.2817\n",
      "Epoch [827/10000], Loss: 26.2015\n",
      "Epoch [828/10000], Loss: 26.1291\n",
      "Epoch [829/10000], Loss: 26.0632\n",
      "Epoch [830/10000], Loss: 26.0032\n",
      "Epoch [831/10000], Loss: 25.9484\n",
      "Epoch [832/10000], Loss: 25.8984\n",
      "Epoch [833/10000], Loss: 25.8524\n",
      "Epoch [834/10000], Loss: 25.7341\n",
      "Epoch [835/10000], Loss: 25.8012\n",
      "Epoch [836/10000], Loss: 26.0394\n",
      "Epoch [837/10000], Loss: 25.9809\n",
      "Epoch [838/10000], Loss: 25.8104\n",
      "Epoch [839/10000], Loss: 26.2807\n",
      "Epoch [840/10000], Loss: 26.1699\n",
      "Epoch [841/10000], Loss: 26.0877\n",
      "Epoch [842/10000], Loss: 26.0134\n",
      "Epoch [843/10000], Loss: 25.9460\n",
      "Epoch [844/10000], Loss: 25.8845\n",
      "Epoch [845/10000], Loss: 25.8282\n",
      "Epoch [846/10000], Loss: 25.7768\n",
      "Epoch [847/10000], Loss: 25.7296\n",
      "Epoch [848/10000], Loss: 25.6862\n",
      "Epoch [849/10000], Loss: 25.5970\n",
      "Epoch [850/10000], Loss: 25.9786\n",
      "Epoch [851/10000], Loss: 25.8881\n",
      "Epoch [852/10000], Loss: 25.8189\n",
      "Epoch [853/10000], Loss: 25.6846\n",
      "Epoch [854/10000], Loss: 26.1205\n",
      "Epoch [855/10000], Loss: 26.0096\n",
      "Epoch [856/10000], Loss: 25.9266\n",
      "Epoch [857/10000], Loss: 25.8520\n",
      "Epoch [858/10000], Loss: 25.7843\n",
      "Epoch [859/10000], Loss: 25.7228\n",
      "Epoch [860/10000], Loss: 25.6666\n",
      "Epoch [861/10000], Loss: 25.6153\n",
      "Epoch [862/10000], Loss: 25.5684\n",
      "Epoch [863/10000], Loss: 25.4425\n",
      "Epoch [864/10000], Loss: 25.8604\n",
      "Epoch [865/10000], Loss: 25.7650\n",
      "Epoch [866/10000], Loss: 25.6936\n",
      "Epoch [867/10000], Loss: 25.5242\n",
      "Epoch [868/10000], Loss: 25.9957\n",
      "Epoch [869/10000], Loss: 25.8802\n",
      "Epoch [870/10000], Loss: 25.7953\n",
      "Epoch [871/10000], Loss: 25.7191\n",
      "Epoch [872/10000], Loss: 25.6501\n",
      "Epoch [873/10000], Loss: 25.5875\n",
      "Epoch [874/10000], Loss: 25.5304\n",
      "Epoch [875/10000], Loss: 25.4784\n",
      "Epoch [876/10000], Loss: 25.4307\n",
      "Epoch [877/10000], Loss: 25.2905\n",
      "Epoch [878/10000], Loss: 25.7239\n",
      "Epoch [879/10000], Loss: 25.6256\n",
      "Epoch [880/10000], Loss: 25.3716\n",
      "Epoch [881/10000], Loss: 25.7081\n",
      "Epoch [882/10000], Loss: 25.6057\n",
      "Epoch [883/10000], Loss: 25.9372\n",
      "Epoch [884/10000], Loss: 25.8083\n",
      "Epoch [885/10000], Loss: 25.7088\n",
      "Epoch [886/10000], Loss: 25.6201\n",
      "Epoch [887/10000], Loss: 25.5406\n",
      "Epoch [888/10000], Loss: 25.4689\n",
      "Epoch [889/10000], Loss: 25.4041\n",
      "Epoch [890/10000], Loss: 25.2437\n",
      "Epoch [891/10000], Loss: 25.6736\n",
      "Epoch [892/10000], Loss: 25.3298\n",
      "Epoch [893/10000], Loss: 25.7786\n",
      "Epoch [894/10000], Loss: 25.5678\n",
      "Epoch [895/10000], Loss: 25.9573\n",
      "Epoch [896/10000], Loss: 25.6433\n",
      "Epoch [897/10000], Loss: 25.9399\n",
      "Epoch [898/10000], Loss: 25.7692\n",
      "Epoch [899/10000], Loss: 25.6501\n",
      "Epoch [900/10000], Loss: 25.5452\n",
      "Epoch [901/10000], Loss: 25.4520\n",
      "Epoch [902/10000], Loss: 25.3737\n",
      "Epoch [903/10000], Loss: 25.6733\n",
      "Epoch [904/10000], Loss: 25.5500\n",
      "Epoch [905/10000], Loss: 25.1539\n",
      "Epoch [906/10000], Loss: 26.1067\n",
      "Epoch [907/10000], Loss: 25.8787\n",
      "Epoch [908/10000], Loss: 25.7387\n",
      "Epoch [909/10000], Loss: 25.4637\n",
      "Epoch [910/10000], Loss: 25.7621\n",
      "Epoch [911/10000], Loss: 25.5946\n",
      "Epoch [912/10000], Loss: 25.4493\n",
      "Epoch [913/10000], Loss: 25.7604\n",
      "Epoch [914/10000], Loss: 25.6114\n",
      "Epoch [915/10000], Loss: 25.4175\n",
      "Epoch [916/10000], Loss: 25.8149\n",
      "Epoch [917/10000], Loss: 25.4650\n",
      "Epoch [918/10000], Loss: 25.8731\n",
      "Epoch [919/10000], Loss: 25.6770\n",
      "Epoch [920/10000], Loss: 25.3630\n",
      "Epoch [921/10000], Loss: 25.6973\n",
      "Epoch [922/10000], Loss: 25.5280\n",
      "Epoch [923/10000], Loss: 25.7842\n",
      "Epoch [924/10000], Loss: 25.6200\n",
      "Epoch [925/10000], Loss: 25.4792\n",
      "Epoch [926/10000], Loss: 25.7964\n",
      "Epoch [927/10000], Loss: 25.3756\n",
      "Epoch [928/10000], Loss: 25.8550\n",
      "Epoch [929/10000], Loss: 25.6459\n",
      "Epoch [930/10000], Loss: 25.5079\n",
      "Epoch [931/10000], Loss: 25.1927\n",
      "Epoch [932/10000], Loss: 25.9259\n",
      "Epoch [933/10000], Loss: 25.6967\n",
      "Epoch [934/10000], Loss: 25.5145\n",
      "Epoch [935/10000], Loss: 25.8376\n",
      "Epoch [936/10000], Loss: 25.6565\n",
      "Epoch [937/10000], Loss: 25.5170\n",
      "Epoch [938/10000], Loss: 25.1909\n",
      "Epoch [939/10000], Loss: 25.6290\n",
      "Epoch [940/10000], Loss: 25.3911\n",
      "Epoch [941/10000], Loss: 25.5039\n",
      "Epoch [942/10000], Loss: 25.8160\n",
      "Epoch [943/10000], Loss: 25.5736\n",
      "Epoch [944/10000], Loss: 25.8780\n",
      "Epoch [945/10000], Loss: 25.6803\n",
      "Epoch [946/10000], Loss: 25.5275\n",
      "Epoch [947/10000], Loss: 25.3931\n",
      "Epoch [948/10000], Loss: 25.1332\n",
      "Epoch [949/10000], Loss: 25.1680\n",
      "Epoch [950/10000], Loss: 25.6936\n",
      "Epoch [951/10000], Loss: 25.3424\n",
      "Epoch [952/10000], Loss: 25.6819\n",
      "Epoch [953/10000], Loss: 25.8980\n",
      "Epoch [954/10000], Loss: 25.6878\n",
      "Epoch [955/10000], Loss: 25.5245\n",
      "Epoch [956/10000], Loss: 25.3815\n",
      "Epoch [957/10000], Loss: 25.0095\n",
      "Epoch [958/10000], Loss: 25.5416\n",
      "Epoch [959/10000], Loss: 25.6871\n",
      "Epoch [960/10000], Loss: 25.4988\n",
      "Epoch [961/10000], Loss: 25.2333\n",
      "Epoch [962/10000], Loss: 25.9329\n",
      "Epoch [963/10000], Loss: 25.6718\n",
      "Epoch [964/10000], Loss: 25.4995\n",
      "Epoch [965/10000], Loss: 25.3495\n",
      "Epoch [966/10000], Loss: 25.1724\n",
      "Epoch [967/10000], Loss: 25.2409\n",
      "Epoch [968/10000], Loss: 25.6904\n",
      "Epoch [969/10000], Loss: 25.4125\n",
      "Epoch [970/10000], Loss: 25.5232\n",
      "Epoch [971/10000], Loss: 25.8463\n",
      "Epoch [972/10000], Loss: 25.5994\n",
      "Epoch [973/10000], Loss: 25.4259\n",
      "Epoch [974/10000], Loss: 25.2023\n",
      "Epoch [975/10000], Loss: 25.5193\n",
      "Epoch [976/10000], Loss: 25.0953\n",
      "Epoch [977/10000], Loss: 25.5862\n",
      "Epoch [978/10000], Loss: 25.4226\n",
      "Epoch [979/10000], Loss: 25.6195\n",
      "Epoch [980/10000], Loss: 25.2744\n",
      "Epoch [981/10000], Loss: 25.5877\n",
      "Epoch [982/10000], Loss: 25.3660\n",
      "Epoch [983/10000], Loss: 25.2314\n",
      "Epoch [984/10000], Loss: 25.4529\n",
      "Epoch [985/10000], Loss: 24.9994\n",
      "Epoch [986/10000], Loss: 25.5241\n",
      "Epoch [987/10000], Loss: 25.7190\n",
      "Epoch [988/10000], Loss: 25.4970\n",
      "Epoch [989/10000], Loss: 25.1532\n",
      "Epoch [990/10000], Loss: 25.4995\n",
      "Epoch [991/10000], Loss: 25.2705\n",
      "Epoch [992/10000], Loss: 25.5064\n",
      "Epoch [993/10000], Loss: 25.3115\n",
      "Epoch [994/10000], Loss: 24.8668\n",
      "Epoch [995/10000], Loss: 25.8338\n",
      "Epoch [996/10000], Loss: 25.5360\n",
      "Epoch [997/10000], Loss: 25.3498\n",
      "Epoch [998/10000], Loss: 25.0059\n",
      "Epoch [999/10000], Loss: 25.3423\n",
      "Epoch [1000/10000], Loss: 25.5449\n",
      "Epoch [1001/10000], Loss: 25.3339\n",
      "Epoch [1002/10000], Loss: 25.1200\n",
      "Epoch [1003/10000], Loss: 25.1908\n",
      "Epoch [1004/10000], Loss: 25.6619\n",
      "Epoch [1005/10000], Loss: 25.3966\n",
      "Epoch [1006/10000], Loss: 25.2197\n",
      "Epoch [1007/10000], Loss: 24.8634\n",
      "Epoch [1008/10000], Loss: 25.6332\n",
      "Epoch [1009/10000], Loss: 25.3607\n",
      "Epoch [1010/10000], Loss: 25.1315\n",
      "Epoch [1011/10000], Loss: 25.4449\n",
      "Epoch [1012/10000], Loss: 25.0065\n",
      "Epoch [1013/10000], Loss: 25.5000\n",
      "Epoch [1014/10000], Loss: 25.2476\n",
      "Epoch [1015/10000], Loss: 24.8308\n",
      "Epoch [1016/10000], Loss: 25.6520\n",
      "Epoch [1017/10000], Loss: 25.3652\n",
      "Epoch [1018/10000], Loss: 25.1362\n",
      "Epoch [1019/10000], Loss: 25.4346\n",
      "Epoch [1020/10000], Loss: 25.2232\n",
      "Epoch [1021/10000], Loss: 24.8481\n",
      "Epoch [1022/10000], Loss: 25.3330\n",
      "Epoch [1023/10000], Loss: 25.0526\n",
      "Epoch [1024/10000], Loss: 25.1059\n",
      "Epoch [1025/10000], Loss: 25.4754\n",
      "Epoch [1026/10000], Loss: 25.1734\n",
      "Epoch [1027/10000], Loss: 25.4651\n",
      "Epoch [1028/10000], Loss: 25.2400\n",
      "Epoch [1029/10000], Loss: 24.7780\n",
      "Epoch [1030/10000], Loss: 25.3443\n",
      "Epoch [1031/10000], Loss: 25.0657\n",
      "Epoch [1032/10000], Loss: 25.3095\n",
      "Epoch [1033/10000], Loss: 24.9080\n",
      "Epoch [1034/10000], Loss: 25.2493\n",
      "Epoch [1035/10000], Loss: 25.4828\n",
      "Epoch [1036/10000], Loss: 25.2456\n",
      "Epoch [1037/10000], Loss: 25.0642\n",
      "Epoch [1038/10000], Loss: 24.6450\n",
      "Epoch [1039/10000], Loss: 25.1615\n",
      "Epoch [1040/10000], Loss: 25.3389\n",
      "Epoch [1041/10000], Loss: 25.1178\n",
      "Epoch [1042/10000], Loss: 24.8113\n",
      "Epoch [1043/10000], Loss: 25.1834\n",
      "Epoch [1044/10000], Loss: 25.2528\n",
      "Epoch [1045/10000], Loss: 25.0690\n",
      "Epoch [1046/10000], Loss: 24.9047\n",
      "Epoch [1047/10000], Loss: 24.5313\n",
      "Epoch [1048/10000], Loss: 25.4272\n",
      "Epoch [1049/10000], Loss: 25.1346\n",
      "Epoch [1050/10000], Loss: 24.7038\n",
      "Epoch [1051/10000], Loss: 25.5765\n",
      "Epoch [1052/10000], Loss: 25.2634\n",
      "Epoch [1053/10000], Loss: 25.0618\n",
      "Epoch [1054/10000], Loss: 24.8887\n",
      "Epoch [1055/10000], Loss: 24.4653\n",
      "Epoch [1056/10000], Loss: 25.4109\n",
      "Epoch [1057/10000], Loss: 25.1086\n",
      "Epoch [1058/10000], Loss: 24.8808\n",
      "Epoch [1059/10000], Loss: 24.9835\n",
      "Epoch [1060/10000], Loss: 25.3431\n",
      "Epoch [1061/10000], Loss: 25.0736\n",
      "Epoch [1062/10000], Loss: 24.8886\n",
      "Epoch [1063/10000], Loss: 24.7178\n",
      "Epoch [1064/10000], Loss: 24.7306\n",
      "Epoch [1065/10000], Loss: 25.2259\n",
      "Epoch [1066/10000], Loss: 24.9148\n",
      "Epoch [1067/10000], Loss: 25.1991\n",
      "Epoch [1068/10000], Loss: 24.7947\n",
      "Epoch [1069/10000], Loss: 25.1788\n",
      "Epoch [1070/10000], Loss: 24.9226\n",
      "Epoch [1071/10000], Loss: 24.7473\n",
      "Epoch [1072/10000], Loss: 24.9722\n",
      "Epoch [1073/10000], Loss: 24.5696\n",
      "Epoch [1074/10000], Loss: 25.0155\n",
      "Epoch [1075/10000], Loss: 25.2403\n",
      "Epoch [1076/10000], Loss: 24.9963\n",
      "Epoch [1077/10000], Loss: 24.6434\n",
      "Epoch [1078/10000], Loss: 25.0282\n",
      "Epoch [1079/10000], Loss: 24.7865\n",
      "Epoch [1080/10000], Loss: 24.9964\n",
      "Epoch [1081/10000], Loss: 24.4994\n",
      "Epoch [1082/10000], Loss: 25.0356\n",
      "Epoch [1083/10000], Loss: 25.2438\n",
      "Epoch [1084/10000], Loss: 24.9900\n",
      "Epoch [1085/10000], Loss: 24.5611\n",
      "Epoch [1086/10000], Loss: 25.0199\n",
      "Epoch [1087/10000], Loss: 24.7959\n",
      "Epoch [1088/10000], Loss: 24.9718\n",
      "Epoch [1089/10000], Loss: 24.7590\n",
      "Epoch [1090/10000], Loss: 24.3351\n",
      "Epoch [1091/10000], Loss: 25.3157\n",
      "Epoch [1092/10000], Loss: 24.9874\n",
      "Epoch [1093/10000], Loss: 24.7847\n",
      "Epoch [1094/10000], Loss: 24.3934\n",
      "Epoch [1095/10000], Loss: 24.8767\n",
      "Epoch [1096/10000], Loss: 24.9900\n",
      "Epoch [1097/10000], Loss: 24.7638\n",
      "Epoch [1098/10000], Loss: 24.2998\n",
      "Epoch [1099/10000], Loss: 25.3166\n",
      "Epoch [1100/10000], Loss: 24.9769\n",
      "Epoch [1101/10000], Loss: 24.7669\n",
      "Epoch [1102/10000], Loss: 24.5043\n",
      "Epoch [1103/10000], Loss: 24.5999\n",
      "Epoch [1104/10000], Loss: 25.0084\n",
      "Epoch [1105/10000], Loss: 24.7383\n",
      "Epoch [1106/10000], Loss: 24.2725\n",
      "Epoch [1107/10000], Loss: 25.2874\n",
      "Epoch [1108/10000], Loss: 24.9411\n",
      "Epoch [1109/10000], Loss: 24.7268\n",
      "Epoch [1110/10000], Loss: 24.5008\n",
      "Epoch [1111/10000], Loss: 24.7638\n",
      "Epoch [1112/10000], Loss: 24.3827\n",
      "Epoch [1113/10000], Loss: 24.7987\n",
      "Epoch [1114/10000], Loss: 24.5581\n",
      "Epoch [1115/10000], Loss: 24.5524\n",
      "Epoch [1116/10000], Loss: 25.0721\n",
      "Epoch [1117/10000], Loss: 24.7672\n",
      "Epoch [1118/10000], Loss: 24.5423\n",
      "Epoch [1119/10000], Loss: 24.7785\n",
      "Epoch [1120/10000], Loss: 24.3120\n",
      "Epoch [1121/10000], Loss: 24.8058\n",
      "Epoch [1122/10000], Loss: 24.5745\n",
      "Epoch [1123/10000], Loss: 24.4613\n",
      "Epoch [1124/10000], Loss: 25.0649\n",
      "Epoch [1125/10000], Loss: 24.7482\n",
      "Epoch [1126/10000], Loss: 24.5544\n",
      "Epoch [1127/10000], Loss: 24.7479\n",
      "Epoch [1128/10000], Loss: 24.5286\n",
      "Epoch [1129/10000], Loss: 24.1458\n",
      "Epoch [1130/10000], Loss: 24.6551\n",
      "Epoch [1131/10000], Loss: 24.7878\n",
      "Epoch [1132/10000], Loss: 24.2822\n",
      "Epoch [1133/10000], Loss: 24.8721\n",
      "Epoch [1134/10000], Loss: 24.5989\n",
      "Epoch [1135/10000], Loss: 24.7707\n",
      "Epoch [1136/10000], Loss: 24.5385\n",
      "Epoch [1137/10000], Loss: 24.0169\n",
      "Epoch [1138/10000], Loss: 25.0279\n",
      "Epoch [1139/10000], Loss: 24.6947\n",
      "Epoch [1140/10000], Loss: 24.1619\n",
      "Epoch [1141/10000], Loss: 24.8136\n",
      "Epoch [1142/10000], Loss: 24.5814\n",
      "Epoch [1143/10000], Loss: 24.7060\n",
      "Epoch [1144/10000], Loss: 24.4755\n",
      "Epoch [1145/10000], Loss: 24.2505\n",
      "Epoch [1146/10000], Loss: 24.4114\n",
      "Epoch [1147/10000], Loss: 24.3717\n",
      "Epoch [1148/10000], Loss: 24.4749\n",
      "Epoch [1149/10000], Loss: 24.0184\n",
      "Epoch [1150/10000], Loss: 25.0122\n",
      "Epoch [1151/10000], Loss: 24.6631\n",
      "Epoch [1152/10000], Loss: 24.4469\n",
      "Epoch [1153/10000], Loss: 24.2639\n",
      "Epoch [1154/10000], Loss: 24.2468\n",
      "Epoch [1155/10000], Loss: 24.7402\n",
      "Epoch [1156/10000], Loss: 24.4526\n",
      "Epoch [1157/10000], Loss: 23.9723\n",
      "Epoch [1158/10000], Loss: 24.9837\n",
      "Epoch [1159/10000], Loss: 24.6264\n",
      "Epoch [1160/10000], Loss: 24.4051\n",
      "Epoch [1161/10000], Loss: 24.2550\n",
      "Epoch [1162/10000], Loss: 24.4615\n",
      "Epoch [1163/10000], Loss: 24.0223\n",
      "Epoch [1164/10000], Loss: 24.5172\n",
      "Epoch [1165/10000], Loss: 23.9362\n",
      "Epoch [1166/10000], Loss: 24.9799\n",
      "Epoch [1167/10000], Loss: 24.6105\n",
      "Epoch [1168/10000], Loss: 24.3037\n",
      "Epoch [1169/10000], Loss: 24.6060\n",
      "Epoch [1170/10000], Loss: 24.3589\n",
      "Epoch [1171/10000], Loss: 23.8980\n",
      "Epoch [1172/10000], Loss: 24.4507\n",
      "Epoch [1173/10000], Loss: 23.9004\n",
      "Epoch [1174/10000], Loss: 24.9143\n",
      "Epoch [1175/10000], Loss: 24.5456\n",
      "Epoch [1176/10000], Loss: 24.2909\n",
      "Epoch [1177/10000], Loss: 24.5416\n",
      "Epoch [1178/10000], Loss: 24.2971\n",
      "Epoch [1179/10000], Loss: 24.1122\n",
      "Epoch [1180/10000], Loss: 23.6621\n",
      "Epoch [1181/10000], Loss: 24.6153\n",
      "Epoch [1182/10000], Loss: 24.0621\n",
      "Epoch [1183/10000], Loss: 24.6343\n",
      "Epoch [1184/10000], Loss: 24.3156\n",
      "Epoch [1185/10000], Loss: 24.5334\n",
      "Epoch [1186/10000], Loss: 24.2796\n",
      "Epoch [1187/10000], Loss: 24.0869\n",
      "Epoch [1188/10000], Loss: 23.6434\n",
      "Epoch [1189/10000], Loss: 24.5895\n",
      "Epoch [1190/10000], Loss: 23.9550\n",
      "Epoch [1191/10000], Loss: 24.6088\n",
      "Epoch [1192/10000], Loss: 24.3150\n",
      "Epoch [1193/10000], Loss: 24.4932\n",
      "Epoch [1194/10000], Loss: 24.2368\n",
      "Epoch [1195/10000], Loss: 24.0411\n",
      "Epoch [1196/10000], Loss: 23.6194\n",
      "Epoch [1197/10000], Loss: 24.5459\n",
      "Epoch [1198/10000], Loss: 24.2231\n",
      "Epoch [1199/10000], Loss: 23.7715\n",
      "Epoch [1200/10000], Loss: 24.4119\n",
      "Epoch [1201/10000], Loss: 24.5047\n",
      "Epoch [1202/10000], Loss: 24.2350\n",
      "Epoch [1203/10000], Loss: 24.0295\n",
      "Epoch [1204/10000], Loss: 23.5996\n",
      "Epoch [1205/10000], Loss: 24.5297\n",
      "Epoch [1206/10000], Loss: 24.1988\n",
      "Epoch [1207/10000], Loss: 23.6707\n",
      "Epoch [1208/10000], Loss: 24.4069\n",
      "Epoch [1209/10000], Loss: 24.4725\n",
      "Epoch [1210/10000], Loss: 24.1985\n",
      "Epoch [1211/10000], Loss: 23.9045\n",
      "Epoch [1212/10000], Loss: 24.1951\n",
      "Epoch [1213/10000], Loss: 23.7669\n",
      "Epoch [1214/10000], Loss: 24.2555\n",
      "Epoch [1215/10000], Loss: 23.8902\n",
      "Epoch [1216/10000], Loss: 23.9511\n",
      "Epoch [1217/10000], Loss: 24.5237\n",
      "Epoch [1218/10000], Loss: 24.1847\n",
      "Epoch [1219/10000], Loss: 23.9296\n",
      "Epoch [1220/10000], Loss: 24.1653\n",
      "Epoch [1221/10000], Loss: 23.6767\n",
      "Epoch [1222/10000], Loss: 24.2250\n",
      "Epoch [1223/10000], Loss: 23.8995\n",
      "Epoch [1224/10000], Loss: 23.8468\n",
      "Epoch [1225/10000], Loss: 24.4898\n",
      "Epoch [1226/10000], Loss: 24.1426\n",
      "Epoch [1227/10000], Loss: 23.9377\n",
      "Epoch [1228/10000], Loss: 24.1152\n",
      "Epoch [1229/10000], Loss: 23.5775\n",
      "Epoch [1230/10000], Loss: 24.1806\n",
      "Epoch [1231/10000], Loss: 23.9003\n",
      "Epoch [1232/10000], Loss: 24.1122\n",
      "Epoch [1233/10000], Loss: 23.6405\n",
      "Epoch [1234/10000], Loss: 24.2385\n",
      "Epoch [1235/10000], Loss: 23.9701\n",
      "Epoch [1236/10000], Loss: 24.1106\n",
      "Epoch [1237/10000], Loss: 23.8691\n",
      "Epoch [1238/10000], Loss: 23.4233\n",
      "Epoch [1239/10000], Loss: 24.0110\n",
      "Epoch [1240/10000], Loss: 24.1392\n",
      "Epoch [1241/10000], Loss: 23.5683\n",
      "Epoch [1242/10000], Loss: 24.2529\n",
      "Epoch [1243/10000], Loss: 23.9964\n",
      "Epoch [1244/10000], Loss: 24.0996\n",
      "Epoch [1245/10000], Loss: 23.8503\n",
      "Epoch [1246/10000], Loss: 23.3606\n",
      "Epoch [1247/10000], Loss: 23.9683\n",
      "Epoch [1248/10000], Loss: 24.0015\n",
      "Epoch [1249/10000], Loss: 23.4590\n",
      "Epoch [1250/10000], Loss: 24.1494\n",
      "Epoch [1251/10000], Loss: 24.2407\n",
      "Epoch [1252/10000], Loss: 23.9570\n",
      "Epoch [1253/10000], Loss: 23.7408\n",
      "Epoch [1254/10000], Loss: 23.2832\n",
      "Epoch [1255/10000], Loss: 24.2859\n",
      "Epoch [1256/10000], Loss: 23.9305\n",
      "Epoch [1257/10000], Loss: 23.7126\n",
      "Epoch [1258/10000], Loss: 23.3057\n",
      "Epoch [1259/10000], Loss: 24.2996\n",
      "Epoch [1260/10000], Loss: 23.9305\n",
      "Epoch [1261/10000], Loss: 23.7057\n",
      "Epoch [1262/10000], Loss: 23.2393\n",
      "Epoch [1263/10000], Loss: 24.2489\n",
      "Epoch [1264/10000], Loss: 23.8868\n",
      "Epoch [1265/10000], Loss: 23.6646\n",
      "Epoch [1266/10000], Loss: 23.2545\n",
      "Epoch [1267/10000], Loss: 24.2542\n",
      "Epoch [1268/10000], Loss: 23.8807\n",
      "Epoch [1269/10000], Loss: 23.5521\n",
      "Epoch [1270/10000], Loss: 23.8784\n",
      "Epoch [1271/10000], Loss: 23.4162\n",
      "Epoch [1272/10000], Loss: 23.9401\n",
      "Epoch [1273/10000], Loss: 23.5648\n",
      "Epoch [1274/10000], Loss: 23.5284\n",
      "Epoch [1275/10000], Loss: 24.1799\n",
      "Epoch [1276/10000], Loss: 23.8209\n",
      "Epoch [1277/10000], Loss: 23.5583\n",
      "Epoch [1278/10000], Loss: 23.8143\n",
      "Epoch [1279/10000], Loss: 23.3094\n",
      "Epoch [1280/10000], Loss: 23.8837\n",
      "Epoch [1281/10000], Loss: 23.5612\n",
      "Epoch [1282/10000], Loss: 23.4169\n",
      "Epoch [1283/10000], Loss: 24.1266\n",
      "Epoch [1284/10000], Loss: 23.7635\n",
      "Epoch [1285/10000], Loss: 23.5625\n",
      "Epoch [1286/10000], Loss: 23.7525\n",
      "Epoch [1287/10000], Loss: 23.2048\n",
      "Epoch [1288/10000], Loss: 23.8302\n",
      "Epoch [1289/10000], Loss: 23.5570\n",
      "Epoch [1290/10000], Loss: 23.7103\n",
      "Epoch [1291/10000], Loss: 23.2154\n",
      "Epoch [1292/10000], Loss: 23.8642\n",
      "Epoch [1293/10000], Loss: 23.5967\n",
      "Epoch [1294/10000], Loss: 23.7475\n",
      "Epoch [1295/10000], Loss: 23.4932\n",
      "Epoch [1296/10000], Loss: 22.9964\n",
      "Epoch [1297/10000], Loss: 24.0148\n",
      "Epoch [1298/10000], Loss: 23.6508\n",
      "Epoch [1299/10000], Loss: 23.1086\n",
      "Epoch [1300/10000], Loss: 23.7632\n",
      "Epoch [1301/10000], Loss: 23.9257\n",
      "Epoch [1302/10000], Loss: 23.6257\n",
      "Epoch [1303/10000], Loss: 23.4019\n",
      "Epoch [1304/10000], Loss: 22.9796\n",
      "Epoch [1305/10000], Loss: 23.9325\n",
      "Epoch [1306/10000], Loss: 23.5729\n",
      "Epoch [1307/10000], Loss: 22.9909\n",
      "Epoch [1308/10000], Loss: 23.7385\n",
      "Epoch [1309/10000], Loss: 23.8546\n",
      "Epoch [1310/10000], Loss: 23.5570\n",
      "Epoch [1311/10000], Loss: 23.3341\n",
      "Epoch [1312/10000], Loss: 22.9615\n",
      "Epoch [1313/10000], Loss: 23.8691\n",
      "Epoch [1314/10000], Loss: 23.5104\n",
      "Epoch [1315/10000], Loss: 23.2894\n",
      "Epoch [1316/10000], Loss: 22.8566\n",
      "Epoch [1317/10000], Loss: 23.9337\n",
      "Epoch [1318/10000], Loss: 23.5448\n",
      "Epoch [1319/10000], Loss: 23.2289\n",
      "Epoch [1320/10000], Loss: 23.2247\n",
      "Epoch [1321/10000], Loss: 23.7981\n",
      "Epoch [1322/10000], Loss: 23.4540\n",
      "Epoch [1323/10000], Loss: 23.1726\n",
      "Epoch [1324/10000], Loss: 23.2047\n",
      "Epoch [1325/10000], Loss: 23.8257\n",
      "Epoch [1326/10000], Loss: 23.4618\n",
      "Epoch [1327/10000], Loss: 23.2209\n",
      "Epoch [1328/10000], Loss: 23.1035\n",
      "Epoch [1329/10000], Loss: 23.7255\n",
      "Epoch [1330/10000], Loss: 23.3804\n",
      "Epoch [1331/10000], Loss: 23.1308\n",
      "Epoch [1332/10000], Loss: 23.0731\n",
      "Epoch [1333/10000], Loss: 23.7612\n",
      "Epoch [1334/10000], Loss: 23.3959\n",
      "Epoch [1335/10000], Loss: 23.2191\n",
      "Epoch [1336/10000], Loss: 23.3546\n",
      "Epoch [1337/10000], Loss: 22.8865\n",
      "Epoch [1338/10000], Loss: 23.4603\n",
      "Epoch [1339/10000], Loss: 23.1616\n",
      "Epoch [1340/10000], Loss: 22.9868\n",
      "Epoch [1341/10000], Loss: 23.7510\n",
      "Epoch [1342/10000], Loss: 23.3297\n",
      "Epoch [1343/10000], Loss: 23.5312\n",
      "Epoch [1344/10000], Loss: 23.2501\n",
      "Epoch [1345/10000], Loss: 22.7766\n",
      "Epoch [1346/10000], Loss: 23.3065\n",
      "Epoch [1347/10000], Loss: 23.5077\n",
      "Epoch [1348/10000], Loss: 22.9148\n",
      "Epoch [1349/10000], Loss: 23.2069\n",
      "Epoch [1350/10000], Loss: 23.3037\n",
      "Epoch [1351/10000], Loss: 23.4181\n",
      "Epoch [1352/10000], Loss: 23.1517\n",
      "Epoch [1353/10000], Loss: 22.6846\n",
      "Epoch [1354/10000], Loss: 23.2902\n",
      "Epoch [1355/10000], Loss: 23.4291\n",
      "Epoch [1356/10000], Loss: 22.7507\n",
      "Epoch [1357/10000], Loss: 23.5090\n",
      "Epoch [1358/10000], Loss: 23.6117\n",
      "Epoch [1359/10000], Loss: 23.2953\n",
      "Epoch [1360/10000], Loss: 23.0573\n",
      "Epoch [1361/10000], Loss: 22.5059\n",
      "Epoch [1362/10000], Loss: 23.6329\n",
      "Epoch [1363/10000], Loss: 23.2424\n",
      "Epoch [1364/10000], Loss: 23.0070\n",
      "Epoch [1365/10000], Loss: 22.5610\n",
      "Epoch [1366/10000], Loss: 23.6315\n",
      "Epoch [1367/10000], Loss: 23.2315\n",
      "Epoch [1368/10000], Loss: 22.9908\n",
      "Epoch [1369/10000], Loss: 22.4746\n",
      "Epoch [1370/10000], Loss: 23.5705\n",
      "Epoch [1371/10000], Loss: 23.1787\n",
      "Epoch [1372/10000], Loss: 22.9417\n",
      "Epoch [1373/10000], Loss: 22.5198\n",
      "Epoch [1374/10000], Loss: 23.5704\n",
      "Epoch [1375/10000], Loss: 23.1688\n",
      "Epoch [1376/10000], Loss: 22.9267\n",
      "Epoch [1377/10000], Loss: 22.4386\n",
      "Epoch [1378/10000], Loss: 23.5097\n",
      "Epoch [1379/10000], Loss: 23.1168\n",
      "Epoch [1380/10000], Loss: 22.8029\n",
      "Epoch [1381/10000], Loss: 22.7765\n",
      "Epoch [1382/10000], Loss: 23.4645\n",
      "Epoch [1383/10000], Loss: 23.0855\n",
      "Epoch [1384/10000], Loss: 22.7532\n",
      "Epoch [1385/10000], Loss: 22.7640\n",
      "Epoch [1386/10000], Loss: 23.3789\n",
      "Epoch [1387/10000], Loss: 23.0170\n",
      "Epoch [1388/10000], Loss: 22.7872\n",
      "Epoch [1389/10000], Loss: 22.6569\n",
      "Epoch [1390/10000], Loss: 23.3805\n",
      "Epoch [1391/10000], Loss: 23.0030\n",
      "Epoch [1392/10000], Loss: 22.7487\n",
      "Epoch [1393/10000], Loss: 22.6439\n",
      "Epoch [1394/10000], Loss: 23.3086\n",
      "Epoch [1395/10000], Loss: 22.8751\n",
      "Epoch [1396/10000], Loss: 23.1097\n",
      "Epoch [1397/10000], Loss: 22.5087\n",
      "Epoch [1398/10000], Loss: 23.2697\n",
      "Epoch [1399/10000], Loss: 22.8161\n",
      "Epoch [1400/10000], Loss: 23.0932\n",
      "Epoch [1401/10000], Loss: 22.4707\n",
      "Epoch [1402/10000], Loss: 23.1733\n",
      "Epoch [1403/10000], Loss: 22.8385\n",
      "Epoch [1404/10000], Loss: 22.9926\n",
      "Epoch [1405/10000], Loss: 22.3936\n",
      "Epoch [1406/10000], Loss: 23.1709\n",
      "Epoch [1407/10000], Loss: 22.7994\n",
      "Epoch [1408/10000], Loss: 23.0027\n",
      "Epoch [1409/10000], Loss: 22.3583\n",
      "Epoch [1410/10000], Loss: 23.0224\n",
      "Epoch [1411/10000], Loss: 23.1540\n",
      "Epoch [1412/10000], Loss: 22.8458\n",
      "Epoch [1413/10000], Loss: 22.2808\n",
      "Epoch [1414/10000], Loss: 22.9730\n",
      "Epoch [1415/10000], Loss: 23.1459\n",
      "Epoch [1416/10000], Loss: 22.8296\n",
      "Epoch [1417/10000], Loss: 22.2308\n",
      "Epoch [1418/10000], Loss: 22.9783\n",
      "Epoch [1419/10000], Loss: 23.0396\n",
      "Epoch [1420/10000], Loss: 22.7427\n",
      "Epoch [1421/10000], Loss: 22.1786\n",
      "Epoch [1422/10000], Loss: 22.9475\n",
      "Epoch [1423/10000], Loss: 23.0582\n",
      "Epoch [1424/10000], Loss: 22.7482\n",
      "Epoch [1425/10000], Loss: 22.4342\n",
      "Epoch [1426/10000], Loss: 22.4464\n",
      "Epoch [1427/10000], Loss: 23.0530\n",
      "Epoch [1428/10000], Loss: 22.6969\n",
      "Epoch [1429/10000], Loss: 22.1014\n",
      "Epoch [1430/10000], Loss: 22.9480\n",
      "Epoch [1431/10000], Loss: 23.0032\n",
      "Epoch [1432/10000], Loss: 22.6912\n",
      "Epoch [1433/10000], Loss: 22.4406\n",
      "Epoch [1434/10000], Loss: 22.3369\n",
      "Epoch [1435/10000], Loss: 23.0012\n",
      "Epoch [1436/10000], Loss: 22.6403\n",
      "Epoch [1437/10000], Loss: 21.9738\n",
      "Epoch [1438/10000], Loss: 23.2900\n",
      "Epoch [1439/10000], Loss: 22.8364\n",
      "Epoch [1440/10000], Loss: 22.4913\n",
      "Epoch [1441/10000], Loss: 22.7379\n",
      "Epoch [1442/10000], Loss: 22.1442\n",
      "Epoch [1443/10000], Loss: 22.8537\n",
      "Epoch [1444/10000], Loss: 22.5075\n",
      "Epoch [1445/10000], Loss: 22.0535\n",
      "Epoch [1446/10000], Loss: 22.7225\n",
      "Epoch [1447/10000], Loss: 22.7138\n",
      "Epoch [1448/10000], Loss: 22.4670\n",
      "Epoch [1449/10000], Loss: 22.6371\n",
      "Epoch [1450/10000], Loss: 22.0371\n",
      "Epoch [1451/10000], Loss: 22.7731\n",
      "Epoch [1452/10000], Loss: 22.3265\n",
      "Epoch [1453/10000], Loss: 22.3371\n",
      "Epoch [1454/10000], Loss: 23.0495\n",
      "Epoch [1455/10000], Loss: 22.5689\n",
      "Epoch [1456/10000], Loss: 22.7863\n",
      "Epoch [1457/10000], Loss: 22.4842\n",
      "Epoch [1458/10000], Loss: 21.9103\n",
      "Epoch [1459/10000], Loss: 22.6602\n",
      "Epoch [1460/10000], Loss: 22.2974\n",
      "Epoch [1461/10000], Loss: 22.2032\n",
      "Epoch [1462/10000], Loss: 22.9544\n",
      "Epoch [1463/10000], Loss: 22.5473\n",
      "Epoch [1464/10000], Loss: 22.6956\n",
      "Epoch [1465/10000], Loss: 22.3990\n",
      "Epoch [1466/10000], Loss: 21.8061\n",
      "Epoch [1467/10000], Loss: 22.5825\n",
      "Epoch [1468/10000], Loss: 22.2855\n",
      "Epoch [1469/10000], Loss: 22.0861\n",
      "Epoch [1470/10000], Loss: 22.8843\n",
      "Epoch [1471/10000], Loss: 22.5374\n",
      "Epoch [1472/10000], Loss: 22.6230\n",
      "Epoch [1473/10000], Loss: 22.3290\n",
      "Epoch [1474/10000], Loss: 22.1081\n",
      "Epoch [1475/10000], Loss: 21.6128\n",
      "Epoch [1476/10000], Loss: 22.7489\n",
      "Epoch [1477/10000], Loss: 21.9378\n",
      "Epoch [1478/10000], Loss: 22.7631\n",
      "Epoch [1479/10000], Loss: 22.8263\n",
      "Epoch [1480/10000], Loss: 22.4778\n",
      "Epoch [1481/10000], Loss: 22.2181\n",
      "Epoch [1482/10000], Loss: 21.6422\n",
      "Epoch [1483/10000], Loss: 22.4313\n",
      "Epoch [1484/10000], Loss: 22.5154\n",
      "Epoch [1485/10000], Loss: 21.8088\n",
      "Epoch [1486/10000], Loss: 22.7034\n",
      "Epoch [1487/10000], Loss: 22.7055\n",
      "Epoch [1488/10000], Loss: 22.3699\n",
      "Epoch [1489/10000], Loss: 22.1185\n",
      "Epoch [1490/10000], Loss: 21.5179\n",
      "Epoch [1491/10000], Loss: 22.7403\n",
      "Epoch [1492/10000], Loss: 22.3221\n",
      "Epoch [1493/10000], Loss: 21.6409\n",
      "Epoch [1494/10000], Loss: 22.9461\n",
      "Epoch [1495/10000], Loss: 22.4750\n",
      "Epoch [1496/10000], Loss: 22.1923\n",
      "Epoch [1497/10000], Loss: 21.9612\n",
      "Epoch [1498/10000], Loss: 21.4934\n",
      "Epoch [1499/10000], Loss: 22.6043\n",
      "Epoch [1500/10000], Loss: 22.2021\n",
      "Epoch [1501/10000], Loss: 21.6096\n",
      "Epoch [1502/10000], Loss: 22.8413\n",
      "Epoch [1503/10000], Loss: 22.3790\n",
      "Epoch [1504/10000], Loss: 22.1012\n",
      "Epoch [1505/10000], Loss: 21.8049\n",
      "Epoch [1506/10000], Loss: 21.7812\n",
      "Epoch [1507/10000], Loss: 22.4680\n",
      "Epoch [1508/10000], Loss: 22.0086\n",
      "Epoch [1509/10000], Loss: 21.9204\n",
      "Epoch [1510/10000], Loss: 22.6962\n",
      "Epoch [1511/10000], Loss: 22.2670\n",
      "Epoch [1512/10000], Loss: 21.9963\n",
      "Epoch [1513/10000], Loss: 21.7961\n",
      "Epoch [1514/10000], Loss: 21.6648\n",
      "Epoch [1515/10000], Loss: 22.3801\n",
      "Epoch [1516/10000], Loss: 21.9941\n",
      "Epoch [1517/10000], Loss: 21.8054\n",
      "Epoch [1518/10000], Loss: 22.6201\n",
      "Epoch [1519/10000], Loss: 22.1908\n",
      "Epoch [1520/10000], Loss: 21.8811\n",
      "Epoch [1521/10000], Loss: 22.1165\n",
      "Epoch [1522/10000], Loss: 21.5106\n",
      "Epoch [1523/10000], Loss: 22.2580\n",
      "Epoch [1524/10000], Loss: 21.9607\n",
      "Epoch [1525/10000], Loss: 21.6798\n",
      "Epoch [1526/10000], Loss: 22.5204\n",
      "Epoch [1527/10000], Loss: 22.0965\n",
      "Epoch [1528/10000], Loss: 21.8717\n",
      "Epoch [1529/10000], Loss: 22.0289\n",
      "Epoch [1530/10000], Loss: 21.4070\n",
      "Epoch [1531/10000], Loss: 22.1527\n",
      "Epoch [1532/10000], Loss: 22.2320\n",
      "Epoch [1533/10000], Loss: 21.5315\n",
      "Epoch [1534/10000], Loss: 22.4006\n",
      "Epoch [1535/10000], Loss: 21.9615\n",
      "Epoch [1536/10000], Loss: 22.1546\n",
      "Epoch [1537/10000], Loss: 21.8475\n",
      "Epoch [1538/10000], Loss: 21.2409\n",
      "Epoch [1539/10000], Loss: 22.4383\n",
      "Epoch [1540/10000], Loss: 22.0061\n",
      "Epoch [1541/10000], Loss: 21.3739\n",
      "Epoch [1542/10000], Loss: 22.2379\n",
      "Epoch [1543/10000], Loss: 21.9039\n",
      "Epoch [1544/10000], Loss: 22.0192\n",
      "Epoch [1545/10000], Loss: 21.7267\n",
      "Epoch [1546/10000], Loss: 21.2380\n",
      "Epoch [1547/10000], Loss: 22.3329\n",
      "Epoch [1548/10000], Loss: 21.9105\n",
      "Epoch [1549/10000], Loss: 21.2540\n",
      "Epoch [1550/10000], Loss: 22.1071\n",
      "Epoch [1551/10000], Loss: 22.1977\n",
      "Epoch [1552/10000], Loss: 21.8549\n",
      "Epoch [1553/10000], Loss: 21.2775\n",
      "Epoch [1554/10000], Loss: 21.9569\n",
      "Epoch [1555/10000], Loss: 21.9499\n",
      "Epoch [1556/10000], Loss: 21.6807\n",
      "Epoch [1557/10000], Loss: 21.0691\n",
      "Epoch [1558/10000], Loss: 22.3846\n",
      "Epoch [1559/10000], Loss: 21.9233\n",
      "Epoch [1560/10000], Loss: 21.6497\n",
      "Epoch [1561/10000], Loss: 21.1763\n",
      "Epoch [1562/10000], Loss: 22.2533\n",
      "Epoch [1563/10000], Loss: 21.8228\n",
      "Epoch [1564/10000], Loss: 21.5609\n",
      "Epoch [1565/10000], Loss: 21.0330\n",
      "Epoch [1566/10000], Loss: 22.2775\n",
      "Epoch [1567/10000], Loss: 21.8230\n",
      "Epoch [1568/10000], Loss: 21.4913\n",
      "Epoch [1569/10000], Loss: 21.4183\n",
      "Epoch [1570/10000], Loss: 22.1124\n",
      "Epoch [1571/10000], Loss: 21.7112\n",
      "Epoch [1572/10000], Loss: 21.0326\n",
      "Epoch [1573/10000], Loss: 22.0143\n",
      "Epoch [1574/10000], Loss: 22.0046\n",
      "Epoch [1575/10000], Loss: 21.6627\n",
      "Epoch [1576/10000], Loss: 21.4415\n",
      "Epoch [1577/10000], Loss: 21.2720\n",
      "Epoch [1578/10000], Loss: 21.9923\n",
      "Epoch [1579/10000], Loss: 21.6015\n",
      "Epoch [1580/10000], Loss: 20.9389\n",
      "Epoch [1581/10000], Loss: 22.2837\n",
      "Epoch [1582/10000], Loss: 21.7965\n",
      "Epoch [1583/10000], Loss: 21.4902\n",
      "Epoch [1584/10000], Loss: 21.6638\n",
      "Epoch [1585/10000], Loss: 21.0712\n",
      "Epoch [1586/10000], Loss: 21.8191\n",
      "Epoch [1587/10000], Loss: 21.4494\n",
      "Epoch [1588/10000], Loss: 20.9424\n",
      "Epoch [1589/10000], Loss: 22.1511\n",
      "Epoch [1590/10000], Loss: 21.5910\n",
      "Epoch [1591/10000], Loss: 21.8061\n",
      "Epoch [1592/10000], Loss: 21.4809\n",
      "Epoch [1593/10000], Loss: 20.9265\n",
      "Epoch [1594/10000], Loss: 21.6894\n",
      "Epoch [1595/10000], Loss: 21.2830\n",
      "Epoch [1596/10000], Loss: 21.2387\n",
      "Epoch [1597/10000], Loss: 21.9863\n",
      "Epoch [1598/10000], Loss: 21.5546\n",
      "Epoch [1599/10000], Loss: 21.6867\n",
      "Epoch [1600/10000], Loss: 21.3726\n",
      "Epoch [1601/10000], Loss: 20.8245\n",
      "Epoch [1602/10000], Loss: 21.5924\n",
      "Epoch [1603/10000], Loss: 21.2660\n",
      "Epoch [1604/10000], Loss: 21.1228\n",
      "Epoch [1605/10000], Loss: 21.9044\n",
      "Epoch [1606/10000], Loss: 21.5430\n",
      "Epoch [1607/10000], Loss: 21.6050\n",
      "Epoch [1608/10000], Loss: 21.2946\n",
      "Epoch [1609/10000], Loss: 20.7318\n",
      "Epoch [1610/10000], Loss: 21.4573\n",
      "Epoch [1611/10000], Loss: 21.5966\n",
      "Epoch [1612/10000], Loss: 20.9693\n",
      "Epoch [1613/10000], Loss: 21.7644\n",
      "Epoch [1614/10000], Loss: 21.7786\n",
      "Epoch [1615/10000], Loss: 21.4178\n",
      "Epoch [1616/10000], Loss: 21.1508\n",
      "Epoch [1617/10000], Loss: 20.6224\n",
      "Epoch [1618/10000], Loss: 21.4107\n",
      "Epoch [1619/10000], Loss: 21.4815\n",
      "Epoch [1620/10000], Loss: 20.8617\n",
      "Epoch [1621/10000], Loss: 21.7268\n",
      "Epoch [1622/10000], Loss: 21.6830\n",
      "Epoch [1623/10000], Loss: 21.3284\n",
      "Epoch [1624/10000], Loss: 21.0650\n",
      "Epoch [1625/10000], Loss: 20.5120\n",
      "Epoch [1626/10000], Loss: 21.7268\n",
      "Epoch [1627/10000], Loss: 21.2883\n",
      "Epoch [1628/10000], Loss: 20.7195\n",
      "Epoch [1629/10000], Loss: 21.9417\n",
      "Epoch [1630/10000], Loss: 21.4465\n",
      "Epoch [1631/10000], Loss: 21.1458\n",
      "Epoch [1632/10000], Loss: 20.4433\n",
      "Epoch [1633/10000], Loss: 21.7786\n",
      "Epoch [1634/10000], Loss: 21.3122\n",
      "Epoch [1635/10000], Loss: 20.6262\n",
      "Epoch [1636/10000], Loss: 21.9503\n",
      "Epoch [1637/10000], Loss: 21.4376\n",
      "Epoch [1638/10000], Loss: 21.1267\n",
      "Epoch [1639/10000], Loss: 20.8769\n",
      "Epoch [1640/10000], Loss: 20.4183\n",
      "Epoch [1641/10000], Loss: 21.5579\n",
      "Epoch [1642/10000], Loss: 21.1286\n",
      "Epoch [1643/10000], Loss: 20.5968\n",
      "Epoch [1644/10000], Loss: 21.7966\n",
      "Epoch [1645/10000], Loss: 21.3048\n",
      "Epoch [1646/10000], Loss: 21.0062\n",
      "Epoch [1647/10000], Loss: 20.3511\n",
      "Epoch [1648/10000], Loss: 21.6471\n",
      "Epoch [1649/10000], Loss: 21.1819\n",
      "Epoch [1650/10000], Loss: 20.5879\n",
      "Epoch [1651/10000], Loss: 21.3368\n",
      "Epoch [1652/10000], Loss: 21.2949\n",
      "Epoch [1653/10000], Loss: 20.9907\n",
      "Epoch [1654/10000], Loss: 20.6688\n",
      "Epoch [1655/10000], Loss: 20.6806\n",
      "Epoch [1656/10000], Loss: 21.3798\n",
      "Epoch [1657/10000], Loss: 20.8993\n",
      "Epoch [1658/10000], Loss: 20.8535\n",
      "Epoch [1659/10000], Loss: 21.6100\n",
      "Epoch [1660/10000], Loss: 21.1504\n",
      "Epoch [1661/10000], Loss: 20.7447\n",
      "Epoch [1662/10000], Loss: 21.0415\n",
      "Epoch [1663/10000], Loss: 20.5027\n",
      "Epoch [1664/10000], Loss: 21.2095\n",
      "Epoch [1665/10000], Loss: 20.8408\n",
      "Epoch [1666/10000], Loss: 20.7121\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[160], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     54\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 55\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     57\u001b[0m     total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     59\u001b[0m loss_now \u001b[39m=\u001b[39m total_loss\u001b[39m/\u001b[39mdataset_size\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[1;32m    133\u001b[0m         group,\n\u001b[1;32m    134\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    139\u001b[0m         state_steps)\n\u001b[0;32m--> 141\u001b[0m     adam(\n\u001b[1;32m    142\u001b[0m         params_with_grad,\n\u001b[1;32m    143\u001b[0m         grads,\n\u001b[1;32m    144\u001b[0m         exp_avgs,\n\u001b[1;32m    145\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    146\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    147\u001b[0m         state_steps,\n\u001b[1;32m    148\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    149\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    150\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    151\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    152\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    153\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    154\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    155\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    156\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    157\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    158\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    159\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/optim/adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 281\u001b[0m func(params,\n\u001b[1;32m    282\u001b[0m      grads,\n\u001b[1;32m    283\u001b[0m      exp_avgs,\n\u001b[1;32m    284\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    285\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    286\u001b[0m      state_steps,\n\u001b[1;32m    287\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    288\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    289\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    290\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    291\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    292\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    293\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    294\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    295\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    296\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    297\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/optim/adam.py:391\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    389\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    390\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 391\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39;49m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    393\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def helps(x):\n",
    "    return (x**2 + 2**x + 1/x)\n",
    "\n",
    "class SimplePerceptron(nn.Module):\n",
    "    def __init__(self, input_size, out_size, hidden_size):\n",
    "        super(SimplePerceptron, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, hidden_size)\n",
    "        self.layer_1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output_layer = nn.Linear(hidden_size, out_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "input_size = 1\n",
    "out_size = 1\n",
    "hidden_size = 64\n",
    "learning_rate = 0.001\n",
    "epochs = 10000\n",
    "dataset_size = 100\n",
    "\n",
    "X = torch.rand(dataset_size, input_size)\n",
    "y = helps(X)\n",
    "\n",
    "print(f\"shape of X: {X.shape}, shape of y: {y.shape}\")\n",
    "\n",
    "model = SimplePerceptron(input_size, out_size, hidden_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    for i in range(dataset_size):\n",
    "        inputs = X[i, :]\n",
    "        labels = y[i].unsqueeze(0)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    loss_now = total_loss/dataset_size\n",
    "    if loss_now < 0.005:\n",
    "        break\n",
    "\n",
    "    # Print the average loss for this epoch\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {total_loss/dataset_size:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ded9ca0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9150])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "54ed789d",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[153], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model(torch\u001b[39m.\u001b[39;49mtensor([\u001b[39m0.5\u001b[39;49m]))[\u001b[39m1\u001b[39;49m,:]\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "model(torch.tensor([0.5]))[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0e9f44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
