{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 99k params\n",
      "tensor([4.1742, 4.1742, 4.1742, 4.1742, 4.1742, 4.1742, 4.1742, 4.1742, 4.1743,\n",
      "        4.1742])\n",
      "tensor([4.1742, 4.1742, 4.1742, 4.1742, 4.1742, 4.1742, 4.1742, 4.1742, 4.1742,\n",
      "        4.1742])\n",
      "Initial training loss: 4.174211025238037, val loss: 4.174208641052246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pavan/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:761: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "\n",
    "inf = torch.inf\n",
    "context_length = 256 # No of tokens\n",
    "model_dim = 64 # dimension of the model -> residual stream\n",
    "n_layers = 2 # no of layers\n",
    "n_heads = 0 # No of attention heads for layer # TODO\n",
    "head_dim = 16\n",
    "vocab_size = 65\n",
    "learning_rate = 0.001\n",
    "max_iters = 0\n",
    "eval_iters = 10\n",
    "batch_size = 64 #Takes 27k iters\n",
    "\n",
    "lower_triangular_matrix = torch.tensor([[1 if i<=j else -torch.inf for i in range(context_length)] for j in range(context_length)]).float()\n",
    "\n",
    "def tokenise(str: str):\n",
    "    return torch.tensor([char_map[i] for i in str])\n",
    "\n",
    "def decode(tokens: list[str]):\n",
    "    return ''.join([reverse_char_map[i] for i in tokens])\n",
    "\n",
    "file = open(\"tiny_shakesphere.txt\", \"r\")\n",
    "full_data = file.read()\n",
    "\n",
    "vocab = list(sorted((set(full_data))))\n",
    "char_map = {vocab[i]: i for i in range(len(vocab))}\n",
    "reverse_char_map = {char_map[i] : i for i in char_map}\n",
    "full_data = tokenise(full_data).tolist()\n",
    "random.shuffle(full_data)\n",
    "full_data = torch.tensor(full_data)\n",
    "\n",
    "total_datapoints  = full_data.shape[0]\n",
    "\n",
    "training_data : list[int] = full_data[:int(total_datapoints*0.9)]\n",
    "validation_data = full_data[int(total_datapoints*0.9):total_datapoints]\n",
    "\n",
    "\n",
    "def sample_data(split: str = \"train\"):\n",
    "    data = training_data if split == 'train' else validation_data\n",
    "    ix = torch.randint(len(data) - context_length, (batch_size,))\n",
    "    x = torch.stack([data[i:i+context_length] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+context_length+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = sample_data(split)\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "        print(losses)\n",
    "    model.train()\n",
    "    \n",
    "    return out[\"train\"], out['val']\n",
    "\n",
    "\n",
    "class Layer(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(model_dim, head_dim)\n",
    "        self.query = nn.Linear(model_dim, head_dim)\n",
    "        self.value = nn.Linear(model_dim, head_dim)\n",
    "        self.proj = nn.Linear(head_dim, model_dim)\n",
    "    \n",
    "    def forward(self, idx):\n",
    "        key = self.key(idx) # (batch, context_length, head_dim)\n",
    "        query = self.query(idx)\n",
    "        value = self.value(idx) # (batch, context_length, head_dim)\n",
    "\n",
    "        attention = (query@torch.transpose(key,1,2))/(math.sqrt(head_dim)) # (batch, context_length, context_length)\n",
    "\n",
    "        attention = torch.tril(attention)\n",
    "\n",
    "        attention = attention.masked_fill(attention == 0, -inf)\n",
    "\n",
    "        attention = F.softmax(attention,-1) # probs along context_length sum to 1\n",
    "\n",
    "        attention_value = attention@value  # (batch, context_length, head_dim)\n",
    "\n",
    "        return self.proj(attention_value)  # (batch, context_length, model_dim)\n",
    "    \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(model_dim, 4*model_dim), nn.Linear(4*model_dim, model_dim))\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, idx):\n",
    "        logits = self.layers(idx)\n",
    "        return self.relu(logits)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, model_dim)\n",
    "        self.pos_embedding = nn.Embedding(context_length, model_dim)\n",
    "\n",
    "        self.attention_layes = nn.ModuleList([AttentionHead() for i in range(n_layers)])\n",
    "        self.mlp_layers = nn.ModuleList([MLP() for i in range(n_layers)])\n",
    "\n",
    "        self.unembed_layer = nn.Linear(model_dim,vocab_size)\n",
    "\n",
    "        self.total_parameters = sum([p.numel() for p in self.parameters()])\n",
    "        print(f\"Model has {self.total_parameters//1000}k params\")\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets = None):\n",
    "        # idx -> [1,2,0,3..] (batch, context_length)\n",
    "\n",
    "        input_sequence_length = idx.shape[-1]\n",
    "\n",
    "        residual_stream = self.token_embedding(idx)  # (batch, context_length, model_dim)\n",
    "        # residual_stream = residual_stream + self.pos_embedding(torch.tensor([i for i in range(input_sequence_length)])) # Pos embedding will be # (context_length, model_dim)\n",
    "        # for i in range(n_layers):\n",
    "        #     residual_stream = residual_stream + self.attention_layes[i](residual_stream)\n",
    "        #     residual_stream = residual_stream + self.attention_layes[i](residual_stream)\n",
    "\n",
    "        residual_stream = self.unembed_layer(residual_stream) # (batch, context_length, vocab_size)\n",
    "        if targets is None:\n",
    "            return residual_stream\n",
    "        (x,y,z) = residual_stream.shape\n",
    "        loss = F.cross_entropy(residual_stream.view(x*y,z), F.one_hot(targets, vocab_size).resize(x*y, z).float())\n",
    "        return residual_stream, loss\n",
    "\n",
    "\n",
    "\n",
    "model = Transformer()\n",
    "train_loss,val_loss = estimate_loss()\n",
    "print(f\"Initial training loss: {train_loss}, val loss: {val_loss}\")\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "X,Y = sample_data()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.1341)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b = X.shape\n",
    "F.cross_entropy(F.one_hot(X,vocab_size).view(a*b,vocab_size).float(), F.one_hot(Y,vocab_size).view(a*b,vocab_size).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([46, 16, 60, 50, 15, 58, 60,  1,  6, 50,  0, 58, 50, 61,  1,  1, 44, 56,\n",
       "        46, 57, 43, 43, 30,  0, 52,  1, 39, 57, 59, 57, 61,  0, 58,  1,  1, 58,\n",
       "        53, 53, 40, 39, 62, 12, 53, 61, 59, 57,  1, 39, 59, 43, 43, 40,  6, 50,\n",
       "        44, 61, 47, 39, 53, 43, 47, 56, 43, 11, 47, 58, 58, 50, 52,  1, 53, 43,\n",
       "        39,  0, 43, 18,  8, 53, 46,  7,  1,  0, 60, 58, 57, 46,  0, 56, 47, 56,\n",
       "        47, 46,  1, 21, 56, 53, 43,  1, 25, 50, 52, 61, 58, 15, 53, 42, 39,  1,\n",
       "         1, 31, 57,  1,  1,  1, 51, 39, 61, 10, 39, 43, 43, 46, 56, 63, 58, 52,\n",
       "        56, 46, 57,  1, 43, 43, 15,  1, 43,  0, 41, 51, 53, 53, 59, 49, 47,  1,\n",
       "        39, 58, 53,  0, 51, 46,  1, 45, 42,  1, 14, 46, 18, 57, 33,  1, 47,  1,\n",
       "        57, 47, 59, 32, 53, 61, 43, 21,  1, 43,  1,  1, 59, 58,  0,  8, 53, 63,\n",
       "         0, 21, 49, 51,  0,  6,  1, 56,  1, 50, 52, 46, 40, 42, 59, 39, 39, 58,\n",
       "        52, 52, 57, 47, 44, 43, 59, 23, 56, 39, 10,  1, 51, 39, 14, 59, 56, 54,\n",
       "         6, 58,  1, 43,  1, 46,  1, 43, 43,  1,  1, 59, 33,  0, 10, 47, 53,  1,\n",
       "         1,  0,  0, 56, 46, 43, 50, 43,  1, 43, 63, 31,  1, 50, 21, 56, 58, 39,\n",
       "        43, 53, 47, 44])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = F.one_hot(x, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 65])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
