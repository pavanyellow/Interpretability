{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 99k params\n",
      "Model has 99k params\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 156\u001b[0m\n\u001b[1;32m    153\u001b[0m     model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(PATH))\n\u001b[1;32m    154\u001b[0m     model\u001b[39m.\u001b[39meval()\n\u001b[0;32m--> 156\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m1\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m    157\u001b[0m train_loss,val_loss \u001b[39m=\u001b[39m estimate_loss()\n\u001b[1;32m    158\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInitial training loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m}\u001b[39;00m\u001b[39m, val loss: \u001b[39m\u001b[39m{\u001b[39;00mval_loss\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "PATH = \"models/pavan_gpt_100k_1.91.bin\"\n",
    "\n",
    "inf = torch.inf\n",
    "context_length = 256 # No of tokens\n",
    "model_dim = 64 # dimension of the model -> residual stream\n",
    "n_layers = 2 # no of layers\n",
    "n_heads = 0 # No of attention heads for layer # TODO\n",
    "head_dim = 16\n",
    "vocab_size = 65\n",
    "learning_rate = 3e-4\n",
    "max_iters = 5000\n",
    "eval_iters = 100\n",
    "batch_size = 32 #Takes 27k iters\n",
    "\n",
    "lower_triangular_matrix = torch.tensor([[1 if i<=j else -torch.inf for i in range(context_length)] for j in range(context_length)]).float()\n",
    "\n",
    "def tokenise(str: str):\n",
    "    return torch.tensor([char_map[i] for i in str])\n",
    "\n",
    "def decode(tokens: list[str]):\n",
    "    return ''.join([reverse_char_map[i] for i in tokens])\n",
    "\n",
    "file = open(\"tiny_shakesphere.txt\", \"r\")\n",
    "full_data = file.read()\n",
    "\n",
    "vocab = list(sorted((set(full_data))))\n",
    "\n",
    "char_map = {vocab[i]: i for i in range(len(vocab))}\n",
    "reverse_char_map = {char_map[i] : i for i in char_map}\n",
    "full_data = tokenise(full_data)\n",
    "\n",
    "total_datapoints  = full_data.shape[0]\n",
    "\n",
    "training_data : list[int] = full_data[:int(total_datapoints*0.9)]\n",
    "validation_data = full_data[int(total_datapoints*0.9):total_datapoints]\n",
    "\n",
    "\n",
    "def sample_data(split: str = \"train\"): # With replacement\n",
    "    data = training_data if split == 'train' else validation_data\n",
    "    ix = torch.randint(len(data) - context_length, (batch_size,))\n",
    "    x = torch.stack([data[i:i+context_length] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+context_length+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = sample_data(split)\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    \n",
    "    return out[\"train\"], out['val']\n",
    "\n",
    "\n",
    "class Layer(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(model_dim, head_dim)\n",
    "        self.query = nn.Linear(model_dim, head_dim)\n",
    "        self.value = nn.Linear(model_dim, head_dim)\n",
    "        self.proj = nn.Linear(head_dim, model_dim)\n",
    "    \n",
    "    def forward(self, idx):\n",
    "        key = self.key(idx) # (batch, context_length, head_dim)\n",
    "        query = self.query(idx)\n",
    "        value = self.value(idx) # (batch, context_length, head_dim)\n",
    "\n",
    "        attention = (query@torch.transpose(key,1,2))/(math.sqrt(head_dim)) # (batch, context_length, context_length)\n",
    "\n",
    "        attention = torch.tril(attention)\n",
    "\n",
    "        attention = attention.masked_fill(attention == 0, -inf)\n",
    "\n",
    "        attention = F.softmax(attention,-1) # probs along context_length sum to 1\n",
    "\n",
    "        attention_value = attention@value  # (batch, context_length, head_dim)\n",
    "\n",
    "        return self.proj(attention_value)  # (batch, context_length, model_dim)\n",
    "    \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(model_dim, 4*model_dim), nn.Linear(4*model_dim, model_dim))\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, idx):\n",
    "        logits = self.layers(idx)\n",
    "        return self.relu(logits)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, model_dim)\n",
    "        self.pos_embedding = nn.Embedding(context_length, model_dim)\n",
    "        self.attention_layes = nn.ModuleList([AttentionHead() for i in range(n_layers)])\n",
    "        self.mlp_layers = nn.ModuleList([MLP() for i in range(n_layers)])\n",
    "        self.unembed_layer = nn.Linear(model_dim,vocab_size)\n",
    "\n",
    "        self.total_parameters = sum([p.numel() for p in self.parameters()])\n",
    "        print(f\"Model has {self.total_parameters//1000}k params\")\n",
    "\n",
    "\n",
    "    def forward(self, idx, targets = None):\n",
    "        # idx -> [1,2,0,3..] (batch, context_length)\n",
    "\n",
    "        # for p in range(idx.shape[0]):\n",
    "        #     print([decode(idx[p].tolist()), decode(targets[p].tolist())])\n",
    "\n",
    "        input_sequence_length = idx.shape[-1]\n",
    "\n",
    "        residual_stream = self.token_embedding(idx)  # (batch, context_length, model_dim)\n",
    "        residual_stream = residual_stream + self.pos_embedding(torch.tensor([i for i in range(input_sequence_length)])) # Pos embedding will be # (context_length, model_dim)\n",
    "        \n",
    "        for i in range(n_layers):\n",
    "            residual_stream = residual_stream + self.attention_layes[i](residual_stream)\n",
    "            residual_stream = residual_stream + self.mlp_layers[i](residual_stream)\n",
    "\n",
    "        residual_stream = self.unembed_layer(residual_stream) # (batch, context_length, vocab_size)\n",
    "        if targets is None:\n",
    "            return residual_stream\n",
    "        (x,y,z) = residual_stream.shape\n",
    "        loss = F.cross_entropy(residual_stream.view(x*y,z), targets.view(x*y))\n",
    "        return residual_stream, loss\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Transformer()\n",
    "\n",
    "LOAD_MODEL = False\n",
    "if LOAD_MODEL:\n",
    "    model = Transformer()\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    model.eval()\n",
    "\n",
    "print(1/0)\n",
    "train_loss,val_loss = estimate_loss()\n",
    "print(f\"Initial training loss: {train_loss}, val loss: {val_loss}\")\n",
    "\n",
    "loss_value = []\n",
    "val_loss_value = []\n",
    "iters = []\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "step_value = max_iters/20\n",
    "start_time = time.time()\n",
    "for iter in range(max_iters):\n",
    "    X,Y= sample_data() # (B, context_length)\n",
    "    logits, loss = model(X, Y)  # (B, context_length, vocab_size)\n",
    "    if iter%step_value ==0:\n",
    "        train_loss,val_loss = estimate_loss()\n",
    "        iters.append(iter)\n",
    "        loss_value.append(train_loss)\n",
    "        val_loss_value.append(val_loss)\n",
    "        print(f\"iter:{iter} training loss: {train_loss}, val loss: {val_loss}\")\n",
    "\n",
    "    \n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "end_time = time.time()\n",
    "print(f\"Took {end_time-start_time}s for {max_iters} epochs\")\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(iters,loss_value, color='blue', label=\"Training\")\n",
    "plt.plot(iters, val_loss_value, \"red\", label = \"validation\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you thick.\n",
      "\n",
      "Gul\n",
      "They by nothy marows and yourew'tre mus'der of grow of kiver of to havom a a strutus wis hive!\n",
      "\n",
      "Sut gime wan in my the do-was, notr borsher alle furt,\n",
      "Lounier, to of you. Haudsed, dida anon Beasse they sely were the eysird\n",
      "I raiur like\n",
      "trine, Aurcke bear is bealos is varcemp,\n",
      "To sones our rie, not all tehour if a hou?\n",
      "If antireld, in arf. Toud you feal ir a rudvereate in hart to life\n",
      "And sove all bust you wil.\n",
      "\n",
      "Trespen of im, you! is nother uin the in the bein oble,\n",
      "Whou of I MI'll, a is have dedl-dighs procet'stipigh for my oftucte.\n",
      "\n",
      "Haucterlive lenotion cor.\n",
      "\n",
      "MAERD INCI:\n",
      "'Tt, of, I far the wherrieds, all may thy wrar.\n",
      "\n",
      "KING O\n",
      "RINCIO:\n",
      "Alad puccar they thou grung bear pan my I and mere chis bard to fill hich his be chom you love thingly the in thou eake:\n",
      "And antr! Hadere Pre it.\n",
      "\n",
      "MERY RICEO:\n",
      "Yood didan:\n",
      "I kis himen kivensse what wackes! our may in ale!\n",
      "\n",
      "NREO:\n",
      "My st and-deatd, and houbbles wild confiringle vive; die;\n",
      "kniry: lich cawgh I the wich bedragion a be noth we Cadve"
     ]
    }
   ],
   "source": [
    "def generate_text(input: str):\n",
    "    max_tokens = 1000\n",
    "    input_tokens = tokenise(input)\n",
    "    print(input, end='')\n",
    "    \n",
    "    for i in range(max_tokens):\n",
    "        now = model(input_tokens.unsqueeze(0))[0][-1]\n",
    "        now = F.softmax(now, dim= 0)\n",
    "        token = torch.multinomial(now,1).item()\n",
    "        input_tokens = torch.tensor(input_tokens.tolist() + [token])\n",
    "        text = decode([token])\n",
    "        print(text, end='')\n",
    "        input_tokens = input_tokens[-context_length:]\n",
    "                \n",
    "\n",
    "generate_text(\"you\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " whis bens,\n",
      "Whis thirgar tink for and by eee acaing ifs fall andoy thow And wommith to theame?\n",
      "\n",
      "Rome hatheut?\n",
      "\n",
      "BENVO:\n",
      "Mill tak. And Romparn the the dell sor thisild,\n",
      "Heake lus nowell theur, who say anto arier this Waite,\n",
      "And arte the orrest their flor eneque?\n",
      "\n",
      "PRICICK:\n",
      "I you\n",
      "Tord, brees Marred thou ancel.\n",
      "\n",
      "'lance; cas and it misess worsly fis.\n",
      "\n",
      "EDWABES:\n",
      "Nure muel\n",
      "Tooder:\n",
      "I her. Ray kide mall bour gis, ase, and is is us cady--y the hese we younor he it slooks hade to meicke:\n",
      "'botion nut\n",
      "ETarids whath. That she stre homse him,\n",
      "Yo migh creast! liftotord your is hive:\n",
      "I will agarin I as beake; and in you ar poner,\n",
      "But in that and thaul aurt bretle fore I propursed\n",
      "CYodes noth are my as him feal,\n",
      "No wregns, and your thall gend eeford.\n",
      "\n",
      "PAPURIANUS:\n",
      "Whe wo worth\n",
      "So kim you faul the thee but the bing\n",
      "Kirne bee!-Hay forcher's as naye his\n",
      "Whicl say both'd reid bive; and the thickel geet\n",
      "Aw o\n",
      "So mor's the proubstervicarting Buse.\n",
      "\n",
      "Ply axtid ye twill firfaires irvettlal yere;\n",
      "Away, where itrances!\n"
     ]
    }
   ],
   "source": [
    "generate_text(\" \")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3337, grad_fn=<DivBackward1>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tokenise(\"alik\").unsqueeze(0)\n",
    "output = model(a)\n",
    "F.cross_entropy(output.resize(4,65), F.one_hot(tokenise(\"like\").unsqueeze(0), vocab_size).resize(4, 65).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"models/pavan_gpt_100k_1.91.bin\"\n",
    "torch.save(model.state_dict(), PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
