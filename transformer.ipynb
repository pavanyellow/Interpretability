{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention torch.Size([5577, 7, 7]) tensor([[ 0.0000,  1.7787, 10.0693,  0.0000,  0.6208,  2.4768,  0.0000],\n",
      "        [11.5818,  7.5673,  1.6659,  0.0000,  8.9626, 14.3081,  7.1469],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  2.7706,  0.0000,  0.0000],\n",
      "        [ 0.0000,  1.2150,  2.9298,  4.1431,  0.0000,  3.0354,  0.0000],\n",
      "        [ 4.5180,  0.0000,  0.0000,  0.0000,  0.0000,  6.6173,  7.0408],\n",
      "        [ 0.0000,  0.0000,  6.4178,  0.0000,  6.5771,  1.3985,  1.7506],\n",
      "        [ 0.3864,  1.9618,  1.4119,  0.0000,  0.0000,  0.0000,  4.9324]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "attention torch.Size([5577, 7, 7]) tensor([[ 0.0000,    -inf,    -inf,     nan,    -inf,    -inf,     nan],\n",
      "        [11.5818,  7.5673,    -inf,     nan,    -inf,    -inf,    -inf],\n",
      "        [ 0.0000,  0.0000,  0.0000,     nan,    -inf,     nan,     nan],\n",
      "        [ 0.0000,  1.2150,  2.9298,  4.1431,     nan,    -inf,     nan],\n",
      "        [ 4.5180,  0.0000,  0.0000,  0.0000,  0.0000,    -inf,    -inf],\n",
      "        [ 0.0000,  0.0000,  6.4178,  0.0000,  6.5771,  1.3985,    -inf],\n",
      "        [ 0.3864,  1.9618,  1.4119,  0.0000,  0.0000,  0.0000,  4.9324]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "attention with softmax tensor([[0.0904, 0.0000, 0.0000,    nan,    nan,    nan,    nan],\n",
      "        [0.3845, 0.3212, 0.0000,    nan,    nan,    nan,    nan],\n",
      "        [0.0904, 0.1247, 0.1456,    nan,    nan,    nan,    nan],\n",
      "        [0.0904, 0.1452, 0.2101,    nan,    nan,    nan,    nan],\n",
      "        [0.1590, 0.1247, 0.1456,    nan,    nan,    nan,    nan],\n",
      "        [0.0904, 0.1247, 0.3249,    nan,    nan,    nan,    nan],\n",
      "        [0.0949, 0.1594, 0.1738,    nan,    nan,    nan,    nan]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "nan\n",
      "attention torch.Size([8365, 7, 7]) tensor([[ 6.5590,  0.0000, 13.8206, 15.0041,  4.9750,  4.6798,  9.1001],\n",
      "        [ 0.0000,  5.8298,  6.0741,  0.0000,  3.5232,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  4.7180,  0.0000,  6.7229, 12.7207],\n",
      "        [ 0.0000,  0.0000,  0.0000,  4.6476,  0.0000,  6.4227,  3.6437],\n",
      "        [ 0.0000,  0.0000,  3.8216,  5.8856,  0.0000,  3.0548,  2.2857],\n",
      "        [ 0.6617,  2.9187, 13.5322,  6.6675,  2.4313,  0.0000,  7.4878],\n",
      "        [ 0.0000,  0.0000,  2.8406,  0.0000,  0.0000,  7.1808,  0.0000]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "attention torch.Size([8365, 7, 7]) tensor([[ 6.5590,     nan,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.0000,  5.8298,    -inf,     nan,    -inf,     nan,     nan],\n",
      "        [ 0.0000,  0.0000,  0.0000,    -inf,     nan,    -inf,    -inf],\n",
      "        [ 0.0000,  0.0000,  0.0000,  4.6476,     nan,    -inf,    -inf],\n",
      "        [ 0.0000,  0.0000,  3.8216,  5.8856,  0.0000,    -inf,    -inf],\n",
      "        [ 0.6617,  2.9187, 13.5322,  6.6675,  2.4313,  0.0000,    -inf],\n",
      "        [ 0.0000,  0.0000,  2.8406,  0.0000,  0.0000,  7.1808,  0.0000]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "attention with softmax tensor([[0.2717,    nan, 0.0000,    nan,    nan,    nan,    nan],\n",
      "        [0.1197,    nan, 0.0000,    nan,    nan,    nan,    nan],\n",
      "        [0.1197,    nan, 0.0955,    nan,    nan,    nan,    nan],\n",
      "        [0.1197,    nan, 0.0955,    nan,    nan,    nan,    nan],\n",
      "        [0.1197,    nan, 0.1541,    nan,    nan,    nan,    nan],\n",
      "        [0.1300,    nan, 0.5186,    nan,    nan,    nan,    nan],\n",
      "        [0.1197,    nan, 0.1363,    nan,    nan,    nan,    nan]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "attention torch.Size([5577, 7, 7]) tensor([[ 0.0000,  1.7787, 10.0693,  0.0000,  0.6208,  2.4768,  0.0000],\n",
      "        [11.5818,  7.5673,  1.6659,  0.0000,  8.9626, 14.3081,  7.1469],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  2.7706,  0.0000,  0.0000],\n",
      "        [ 0.0000,  1.2150,  2.9298,  4.1431,  0.0000,  3.0354,  0.0000],\n",
      "        [ 4.5180,  0.0000,  0.0000,  0.0000,  0.0000,  6.6173,  7.0408],\n",
      "        [ 0.0000,  0.0000,  6.4178,  0.0000,  6.5771,  1.3985,  1.7506],\n",
      "        [ 0.3864,  1.9618,  1.4119,  0.0000,  0.0000,  0.0000,  4.9324]])\n",
      "attention torch.Size([5577, 7, 7]) tensor([[ 0.0000,    -inf,    -inf,     nan,    -inf,    -inf,     nan],\n",
      "        [11.5818,  7.5673,    -inf,     nan,    -inf,    -inf,    -inf],\n",
      "        [ 0.0000,  0.0000,  0.0000,     nan,    -inf,     nan,     nan],\n",
      "        [ 0.0000,  1.2150,  2.9298,  4.1431,     nan,    -inf,     nan],\n",
      "        [ 4.5180,  0.0000,  0.0000,  0.0000,  0.0000,    -inf,    -inf],\n",
      "        [ 0.0000,  0.0000,  6.4178,  0.0000,  6.5771,  1.3985,    -inf],\n",
      "        [ 0.3864,  1.9618,  1.4119,  0.0000,  0.0000,  0.0000,  4.9324]])\n",
      "attention with softmax tensor([[0.0904, 0.0000, 0.0000,    nan,    nan,    nan,    nan],\n",
      "        [0.3845, 0.3212, 0.0000,    nan,    nan,    nan,    nan],\n",
      "        [0.0904, 0.1247, 0.1456,    nan,    nan,    nan,    nan],\n",
      "        [0.0904, 0.1452, 0.2101,    nan,    nan,    nan,    nan],\n",
      "        [0.1590, 0.1247, 0.1456,    nan,    nan,    nan,    nan],\n",
      "        [0.0904, 0.1247, 0.3249,    nan,    nan,    nan,    nan],\n",
      "        [0.0949, 0.1594, 0.1738,    nan,    nan,    nan,    nan]])\n",
      "iter:0 training loss: nan, val loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pavan/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:761: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention torch.Size([8365, 7, 7]) tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "attention torch.Size([8365, 7, 7]) tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "attention with softmax tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "attention torch.Size([8365, 7, 7]) tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "attention torch.Size([8365, 7, 7]) tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "attention with softmax tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "attention torch.Size([8365, 7, 7]) tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "attention torch.Size([8365, 7, 7]) tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "attention with softmax tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "attention torch.Size([8365, 7, 7]) tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "attention torch.Size([8365, 7, 7]) tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "attention with softmax tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "attention torch.Size([8365, 7, 7]) tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "attention torch.Size([8365, 7, 7]) tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "attention with softmax tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "attention torch.Size([5577, 7, 7]) tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]])\n",
      "attention torch.Size([5577, 7, 7]) tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]])\n",
      "attention with softmax tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]])\n",
      "iter:5 training loss: nan, val loss: nan\n",
      "attention torch.Size([8365, 7, 7]) tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "attention torch.Size([8365, 7, 7]) tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "attention with softmax tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "attention torch.Size([8365, 7, 7]) tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "attention torch.Size([8365, 7, 7]) tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "attention with softmax tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "attention torch.Size([8365, 7, 7]) tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "attention torch.Size([8365, 7, 7]) tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "attention with softmax tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "attention torch.Size([8365, 7, 7]) tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "attention torch.Size([8365, 7, 7]) tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "attention with softmax tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "attention torch.Size([8365, 7, 7]) tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "attention torch.Size([8365, 7, 7]) tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "attention with softmax tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "attention torch.Size([5577, 7, 7]) tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]])\n",
      "attention torch.Size([5577, 7, 7]) tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]])\n",
      "attention with softmax tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]])\n",
      "iter:10 training loss: nan, val loss: nan\n",
      "attention torch.Size([8365, 7, 7]) tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "attention torch.Size([8365, 7, 7]) tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "attention with softmax tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "attention torch.Size([8365, 7, 7]) tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "attention torch.Size([8365, 7, 7]) tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "attention with softmax tensor([[nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan]], grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 169\u001b[0m\n\u001b[1;32m    165\u001b[0m         model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m    168\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad(set_to_none\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 169\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    170\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    171\u001b[0m end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "\n",
    "inf = torch.inf\n",
    "context_length = 8 # No of tokens\n",
    "model_dim = 64 # dimension of the model -> residual stream\n",
    "n_layers = 2 # no of layers\n",
    "n_heads = 2 # No of attention heads for layer\n",
    "head_dim = 8\n",
    "vocab_size = 65\n",
    "learning_rate = 0.01\n",
    "max_iters = 100\n",
    "\n",
    "lower_triangular_matrix = torch.tensor([[1 if i<=j else -torch.inf for i in range(7)] for j in range(7)]).float()\n",
    "\n",
    "\n",
    "\n",
    "def sample_training_data():\n",
    "    X = training_data[:,:-1]\n",
    "    Y = training_data[:,1:] \n",
    "    return X,Y\n",
    "\n",
    "torch.no_grad()\n",
    "def get_val_loss():\n",
    "    X = validation_data[:,:-1]\n",
    "    Y = validation_data[:,1:]\n",
    "    _, loss = model(X, Y)  # (B, context_length, vocab_size)\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(model_dim, model_dim)\n",
    "        self.query = nn.Linear(model_dim, model_dim)\n",
    "        self.value = nn.Linear(model_dim, model_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, idx):\n",
    "        # idx -> (batch, context_length, model_dim)\n",
    "\n",
    "        # (x,y,_) = idx.shape\n",
    "        # for i in range(x):\n",
    "        #     for j in range(y):\n",
    "        #         for k in range(j):\n",
    "        #             if j!=k:\n",
    "        #                 idx[i][j]+=idx[i][k]\n",
    "        #         if j>=1:\n",
    "        #             idx[i][j]/=(j)\n",
    "        # return idx\n",
    "\n",
    "        # return lower_triangular_matrix@idx\n",
    "\n",
    "        key = self.key(idx) # (batch, context_length, head_dim)\n",
    "        query = self.query(idx)\n",
    "        value = self.value(idx) # (batch, context_length, head_dim)\n",
    "\n",
    "        attention = query@torch.transpose(key,1,2) # (batch, context_length, context_length)\n",
    "        attention =torch.tril(attention)\n",
    "        print(\"attention\",attention.shape, attention[0,:])\n",
    "\n",
    "\n",
    "        attention = attention*lower_triangular_matrix\n",
    "\n",
    "        print(\"attention\",attention.shape, attention[0,:])\n",
    "\n",
    "        attention = F.softmax((attention/math.sqrt(model_dim)),1) # probs along context_length sum to 1\n",
    "        print(\"attention with softmax\", attention[0,:])\n",
    "\n",
    "        return attention@value  # (batch, context_length, model_dim)\n",
    "    \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(model_dim, model_dim), nn.Linear(model_dim, model_dim))\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, idx):\n",
    "        logits = self.layers(idx)\n",
    "        return self.relu(logits)\n",
    "\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, model_dim)\n",
    "        self.pos_embedding = nn.Embedding(context_length-1, model_dim)\n",
    "        self.attention_head = AttentionHead()\n",
    "        \n",
    "        self.mlp = MLP()\n",
    "\n",
    "        self.unembed_layer = nn.Linear(model_dim,vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets = None):\n",
    "        # idx -> [1,2,0,3..] (batch, context_length)\n",
    "\n",
    "\n",
    "        residual_stream = self.token_embedding(idx)  # (batch, context_length, model_dim)\n",
    "        residual_stream = residual_stream + self.pos_embedding(torch.tensor([i for i in range(context_length-1)])) # Pos embedding will be # (context_length, model_dim)\n",
    "        \n",
    "        residual_stream= residual_stream + self.attention_head(residual_stream)\n",
    "\n",
    "        residual_stream= residual_stream + self.mlp(residual_stream) \n",
    "\n",
    "        residual_stream = self.unembed_layer(residual_stream) # (batch, context_length, vocab_size)\n",
    "        if targets is None:\n",
    "            return residual_stream\n",
    "        (x,y,z) = residual_stream.shape\n",
    "        loss = F.cross_entropy(residual_stream.resize(x*y,z), F.one_hot(targets, vocab_size).resize(x*y, z).float())\n",
    "        return residual_stream, loss\n",
    "\n",
    "def tokenise(str: str):\n",
    "    return torch.tensor([char_map[i] for i in str])\n",
    "\n",
    "\n",
    "file = open(\"tiny_shakesphere.txt\", \"r\")\n",
    "full_data = file.read()\n",
    "\n",
    "vocab = list(sorted((set(full_data))))\n",
    "\n",
    "char_map = {vocab[i]: i for i in range(len(vocab))}\n",
    "full_data = tokenise(full_data)\n",
    "\n",
    "full_data = full_data[:len(full_data)- len(full_data)%context_length].reshape(-1,context_length) # Make it multiple of context length\n",
    "\n",
    "random.shuffle(full_data)\n",
    "total_datapoints  = full_data.shape[0]//10\n",
    "training_data = full_data[:int(total_datapoints*0.6), :]\n",
    "validation_data = full_data[int(total_datapoints*0.6):total_datapoints,:]\n",
    "\n",
    "\n",
    "\n",
    "model = Transformer()\n",
    "print(get_val_loss())\n",
    "loss_value = []\n",
    "val_loss_value = []\n",
    "iters = []\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "step_value = max_iters/20\n",
    "start_time = time.time()\n",
    "for iter in range(max_iters):\n",
    "    X,Y= sample_training_data() # (B, context_length)\n",
    "    logits, loss = model(X, Y)  # (B, context_length, vocab_size)\n",
    "    if iter%step_value ==0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = get_val_loss()\n",
    "            iters.append(iter)\n",
    "            loss_value.append(loss.item())\n",
    "            val_loss_value.append(val_loss)\n",
    "            print(f\"iter:{iter} training loss: {loss.item()}, val loss: {val_loss}\")\n",
    "        model.train()\n",
    "\n",
    "    \n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "end_time = time.time()\n",
    "print(f\"Took {end_time-start_time}s for {max_iters} epochs\")\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(iters,loss_value, color='blue', label=\"Training\")\n",
    "plt.plot(iters, val_loss_value, \"red\", label = \"validation\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000],\n",
       "        [0.5077, 0.4923, 0.0000],\n",
       "        [0.4229, 0.3701, 0.2071]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.softmax(attn,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139424"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '$',\n",
       " '&',\n",
       " \"'\",\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '3',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sorted((set(data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
