{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 1236k params\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 159\u001b[0m\n\u001b[1;32m    156\u001b[0m     model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(PATH))\n\u001b[1;32m    157\u001b[0m     model\u001b[39m.\u001b[39meval()\n\u001b[0;32m--> 159\u001b[0m train_loss,val_loss \u001b[39m=\u001b[39m estimate_loss()\n\u001b[1;32m    160\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInitial training loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m}\u001b[39;00m\u001b[39m, val loss: \u001b[39m\u001b[39m{\u001b[39;00mval_loss\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    162\u001b[0m loss_value \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[1], line 64\u001b[0m, in \u001b[0;36mestimate_loss\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(eval_iters):\n\u001b[1;32m     63\u001b[0m     X, Y \u001b[39m=\u001b[39m sample_data(split)\n\u001b[0;32m---> 64\u001b[0m     _, loss \u001b[39m=\u001b[39m model(X, Y)\n\u001b[1;32m     65\u001b[0m     losses[k] \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     66\u001b[0m out[split] \u001b[39m=\u001b[39m losses\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[1], line 140\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    137\u001b[0m residual_stream \u001b[39m=\u001b[39m residual_stream \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_embedding(torch\u001b[39m.\u001b[39mtensor([i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(input_sequence_length)])) \u001b[39m# Pos embedding will be # (context_length, model_dim)\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_layers):\n\u001b[0;32m--> 140\u001b[0m     residual_stream \u001b[39m=\u001b[39m residual_stream \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention_layes[i](residual_stream)\n\u001b[1;32m    141\u001b[0m     residual_stream \u001b[39m=\u001b[39m residual_stream \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp_layers[i](residual_stream)\n\u001b[1;32m    143\u001b[0m residual_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munembed_layer(residual_stream) \u001b[39m# (batch, context_length, vocab_size)\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[1], line 93\u001b[0m, in \u001b[0;36mAttentionHead.forward\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     89\u001b[0m value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue(idx) \u001b[39m# (batch, context_length, head_dim)\u001b[39;00m\n\u001b[1;32m     91\u001b[0m attention \u001b[39m=\u001b[39m (query\u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mtranspose(key,\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m))\u001b[39m/\u001b[39m(math\u001b[39m.\u001b[39msqrt(head_dim)) \u001b[39m# (batch, context_length, context_length)\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m attention \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtril(attention)\n\u001b[1;32m     95\u001b[0m attention \u001b[39m=\u001b[39m attention\u001b[39m.\u001b[39mmasked_fill(attention \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m-\u001b[39minf)\n\u001b[1;32m     97\u001b[0m attention \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(attention,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m# probs along context_length sum to 1\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "PATH = \"models/pavan_gpt_100k_1.91.bin\"\n",
    "LOAD_MODEL = False\n",
    "\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "inf = torch.inf\n",
    "context_length = 256 # No of tokens\n",
    "model_dim = 128 # dimension of the model -> residual stream\n",
    "n_layers = 6 # no of layers\n",
    "n_heads = 0 # No of attention heads for layer # TODO\n",
    "head_dim = 128\n",
    "vocab_size = 65\n",
    "learning_rate = 3e-4\n",
    "max_iters = 5000\n",
    "eval_iters = 100\n",
    "batch_size = 32 #Takes 27k iters\n",
    "\n",
    "lower_triangular_matrix = torch.tensor([[1 if i<=j else -torch.inf for i in range(context_length)] for j in range(context_length)]).float()\n",
    "\n",
    "def tokenise(str: str):\n",
    "    return torch.tensor([char_map[i] for i in str])\n",
    "\n",
    "def decode(tokens: list[str]):\n",
    "    return ''.join([reverse_char_map[i] for i in tokens])\n",
    "\n",
    "file = open(\"tiny_shakesphere.txt\", \"r\")\n",
    "full_data = file.read()\n",
    "\n",
    "vocab = list(sorted((set(full_data))))\n",
    "\n",
    "char_map = {vocab[i]: i for i in range(len(vocab))}\n",
    "reverse_char_map = {char_map[i] : i for i in char_map}\n",
    "full_data = tokenise(full_data)\n",
    "\n",
    "total_datapoints  = full_data.shape[0]\n",
    "\n",
    "training_data : list[int] = full_data[:int(total_datapoints*0.9)]\n",
    "validation_data = full_data[int(total_datapoints*0.9):total_datapoints]\n",
    "\n",
    "\n",
    "def sample_data(split: str = \"train\"): # With replacement\n",
    "    data = training_data if split == 'train' else validation_data\n",
    "    ix = torch.randint(len(data) - context_length, (batch_size,))\n",
    "    x = torch.stack([data[i:i+context_length] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+context_length+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = sample_data(split)\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    \n",
    "    return out[\"train\"], out['val']\n",
    "\n",
    "\n",
    "class Layer(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(model_dim, head_dim)\n",
    "        self.query = nn.Linear(model_dim, head_dim)\n",
    "        self.value = nn.Linear(model_dim, head_dim)\n",
    "        self.proj = nn.Linear(head_dim, model_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, idx):\n",
    "        key = self.key(idx) # (batch, context_length, head_dim)\n",
    "        query = self.query(idx)\n",
    "        value = self.value(idx) # (batch, context_length, head_dim)\n",
    "\n",
    "        attention = (query@torch.transpose(key,1,2))/(math.sqrt(head_dim)) # (batch, context_length, context_length)\n",
    "\n",
    "        attention = torch.tril(attention)\n",
    "\n",
    "        attention = attention.masked_fill(attention == 0, -inf)\n",
    "\n",
    "        attention = F.softmax(attention,-1) # probs along context_length sum to 1\n",
    "\n",
    "        attention_value = attention@value  # (batch, context_length, head_dim)\n",
    "\n",
    "        attention_value = self.proj(attention_value)  # (batch, context_length, model_dim)\n",
    "        return self.dropout(attention_value)\n",
    "    \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(model_dim, 4*model_dim), nn.ReLU(), nn.Linear(4*model_dim, model_dim))\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, idx):\n",
    "        logits = self.layers(idx)\n",
    "        return self.dropout(logits)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, model_dim)\n",
    "        self.pos_embedding = nn.Embedding(context_length, model_dim)\n",
    "        self.attention_layes = nn.ModuleList([AttentionHead() for i in range(n_layers)])\n",
    "        self.mlp_layers = nn.ModuleList([MLP() for i in range(n_layers)])\n",
    "        self.unembed_layer = nn.Linear(model_dim,vocab_size)\n",
    "\n",
    "        self.total_parameters = sum([p.numel() for p in self.parameters()])\n",
    "        print(f\"Model has {self.total_parameters//1000}k params\")\n",
    "\n",
    "\n",
    "    def forward(self, idx, targets = None):\n",
    "        # idx -> [1,2,0,3..] (batch, context_length)\n",
    "\n",
    "        # for p in range(idx.shape[0]):\n",
    "        #     print([decode(idx[p].tolist()), decode(targets[p].tolist())])\n",
    "\n",
    "        input_sequence_length = idx.shape[-1]\n",
    "\n",
    "        residual_stream = self.token_embedding(idx)  # (batch, context_length, model_dim)\n",
    "        residual_stream = residual_stream + self.pos_embedding(torch.tensor([i for i in range(input_sequence_length)])) # Pos embedding will be # (context_length, model_dim)\n",
    "        \n",
    "        for i in range(n_layers):\n",
    "            residual_stream = residual_stream + self.attention_layes[i](residual_stream)\n",
    "            residual_stream = residual_stream + self.mlp_layers[i](residual_stream)\n",
    "\n",
    "        residual_stream = self.unembed_layer(residual_stream) # (batch, context_length, vocab_size)\n",
    "        if targets is None:\n",
    "            return residual_stream\n",
    "        (x,y,z) = residual_stream.shape\n",
    "        loss = F.cross_entropy(residual_stream.view(x*y,z), targets.view(x*y))\n",
    "        return residual_stream, loss\n",
    "    \n",
    "\n",
    "model = Transformer()\n",
    "\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    model = Transformer()\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    model.eval()\n",
    "\n",
    "train_loss,val_loss = estimate_loss()\n",
    "print(f\"Initial training loss: {train_loss}, val loss: {val_loss}\")\n",
    "\n",
    "loss_value = []\n",
    "val_loss_value = []\n",
    "iters = []\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "step_value = max_iters/20\n",
    "start_time = time.time()\n",
    "for iter in range(max_iters):\n",
    "    X,Y= sample_data() # (B, context_length)\n",
    "    logits, loss = model(X, Y)  # (B, context_length, vocab_size)\n",
    "    if iter%step_value ==0:\n",
    "        train_loss,val_loss = estimate_loss()\n",
    "        iters.append(iter)\n",
    "        loss_value.append(train_loss)\n",
    "        val_loss_value.append(val_loss)\n",
    "        print(f\"iter:{iter} training loss: {train_loss}, val loss: {val_loss}\")\n",
    "\n",
    "    \n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "end_time = time.time()\n",
    "print(f\"Took {end_time-start_time}s for {max_iters} epochs\")\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(iters,loss_value, color='blue', label=\"Training\")\n",
    "plt.plot(iters, val_loss_value, \"red\", label = \"validation\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your lord;\n",
      "From your from my faulters.\n",
      "\n",
      "herredness on my and bears:\n",
      "Bell though trel'd topil'd cortain not swould or\n",
      "Littleman tor lass of the strongs hone;\n",
      "Now and answer.\n",
      "\n",
      "First Secondier: well, by crabelandamp'd you not? forse?\n",
      "\n",
      "Epioram,\n",
      "God bell, the alse your which obth'ts you,\n",
      "More for his comforthersomenly die,\n",
      "Cought mother or it, Bukchil'd he grosery?'\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "No. I, to not you be many.\n",
      "\n",
      "POMPEY:\n",
      "Go, homon is be Coase have you cbare,\n",
      "Or led, true and edgrille, my orrackly to age,\n",
      "Werel twhen not say, not to hour flitter'd gary\n",
      "Thou lie From For\n",
      "chento worse chride pherinon\n",
      "Is thee of a' thine: he chountie! Speet to ie at are our than and reclorine,\n",
      "Andwelcome have plectate.'\n",
      "Chown, palys tekunctio, my indid,\n",
      "Why sidon flow, of honds with that\n",
      "Lord himself; and be his ear the Ciriall:\n",
      "Go this must sir?\n",
      "\n",
      "GAUCESSTUS:\n",
      "Then thee; and crovedles tence hate,\n",
      "And seech them, this know\n",
      "za, sir, sagancinmon'd tend, till ques.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Low not not and evilt her,\n",
      "I'll ats in t"
     ]
    }
   ],
   "source": [
    "def generate_text(input: str):\n",
    "    max_tokens = 1000\n",
    "    input_tokens = tokenise(input)\n",
    "    print(input, end='')\n",
    "    \n",
    "    for i in range(max_tokens):\n",
    "        now = model(input_tokens.unsqueeze(0))[0][-1]\n",
    "        now = F.softmax(now, dim= 0)\n",
    "        token = torch.multinomial(now,1).item()\n",
    "        input_tokens = torch.tensor(input_tokens.tolist() + [token])\n",
    "        text = decode([token])\n",
    "        print(text, end='')\n",
    "        input_tokens = input_tokens[-context_length:]\n",
    "                \n",
    "\n",
    "generate_text(\"you\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " whis bens,\n",
      "Whis thirgar tink for and by eee acaing ifs fall andoy thow And wommith to theame?\n",
      "\n",
      "Rome hatheut?\n",
      "\n",
      "BENVO:\n",
      "Mill tak. And Romparn the the dell sor thisild,\n",
      "Heake lus nowell theur, who say anto arier this Waite,\n",
      "And arte the orrest their flor eneque?\n",
      "\n",
      "PRICICK:\n",
      "I you\n",
      "Tord, brees Marred thou ancel.\n",
      "\n",
      "'lance; cas and it misess worsly fis.\n",
      "\n",
      "EDWABES:\n",
      "Nure muel\n",
      "Tooder:\n",
      "I her. Ray kide mall bour gis, ase, and is is us cady--y the hese we younor he it slooks hade to meicke:\n",
      "'botion nut\n",
      "ETarids whath. That she stre homse him,\n",
      "Yo migh creast! liftotord your is hive:\n",
      "I will agarin I as beake; and in you ar poner,\n",
      "But in that and thaul aurt bretle fore I propursed\n",
      "CYodes noth are my as him feal,\n",
      "No wregns, and your thall gend eeford.\n",
      "\n",
      "PAPURIANUS:\n",
      "Whe wo worth\n",
      "So kim you faul the thee but the bing\n",
      "Kirne bee!-Hay forcher's as naye his\n",
      "Whicl say both'd reid bive; and the thickel geet\n",
      "Aw o\n",
      "So mor's the proubstervicarting Buse.\n",
      "\n",
      "Ply axtid ye twill firfaires irvettlal yere;\n",
      "Away, where itrances!\n"
     ]
    }
   ],
   "source": [
    "generate_text(\" \")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3337, grad_fn=<DivBackward1>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tokenise(\"alik\").unsqueeze(0)\n",
    "output = model(a)\n",
    "F.cross_entropy(output.resize(4,65), F.one_hot(tokenise(\"like\").unsqueeze(0), vocab_size).resize(4, 65).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"models/pavan_gpt_1M_1.48.bin\"\n",
    "torch.save(model.state_dict(), PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
