{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 994k params\n",
      "Initial training loss: 3.872518539428711, val loss: 3.8691565990448\n",
      "iter:0 training loss: 3.867757558822632, val loss: 3.877835750579834\n",
      "iter:100 training loss: 1.689276099205017, val loss: 1.689246654510498\n",
      "iter:200 training loss: 1.6728347539901733, val loss: 1.671871542930603\n",
      "iter:300 training loss: 1.6674424409866333, val loss: 1.6696048974990845\n",
      "iter:400 training loss: 1.6652148962020874, val loss: 1.661490797996521\n",
      "iter:500 training loss: 1.659629225730896, val loss: 1.6613003015518188\n",
      "iter:600 training loss: 1.6514853239059448, val loss: 1.6544275283813477\n",
      "iter:700 training loss: 1.479253888130188, val loss: 1.4778386354446411\n",
      "iter:800 training loss: 1.2783411741256714, val loss: 1.2783474922180176\n",
      "iter:900 training loss: 0.5855996608734131, val loss: 0.5959317684173584\n",
      "iter:1000 training loss: 0.14557722210884094, val loss: 0.14054235816001892\n",
      "iter:1100 training loss: 0.023269880563020706, val loss: 0.023497002199292183\n",
      "iter:1200 training loss: 0.008709314279258251, val loss: 0.007654755841940641\n",
      "iter:1300 training loss: 0.011757919564843178, val loss: 0.012303348630666733\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 203\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39miter:\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39miter\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m training loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m}\u001b[39;00m\u001b[39m, val loss: \u001b[39m\u001b[39m{\u001b[39;00mval_loss\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    202\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad(set_to_none\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 203\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    204\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    206\u001b[0m end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "\n",
    "PATH = \"models/addition_1M.bin\"\n",
    "LOAD_MODEL = False\n",
    "\n",
    "inf = torch.inf\n",
    "\n",
    "max_digits = 6 # 0 to 10**(x -1)\n",
    "context_length = 3*max_digits + 2 # {max_digits+max_digits=max_digits}\n",
    "model_dim = 128 # dimension of the model -> residual stream\n",
    "n_layers = 6 # no of layers\n",
    "vocab_size = 14\n",
    "head_dim = 64\n",
    "learning_rate = 3e-4\n",
    "max_iters = 2000\n",
    "eval_iters = 100\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "\n",
    "lower_triangular_matrix = torch.tensor([[1 if i<=j else -torch.inf for i in range(context_length)] for j in range(context_length)]).float()\n",
    "\n",
    "def tokenise(str: str):\n",
    "    tokens = []\n",
    "    for i in list(str):\n",
    "        if i in set(list(\"0123456789\")):\n",
    "            tokens.append(int(i))\n",
    "        if i == \"+\":\n",
    "            tokens.append(11)\n",
    "        if i == \"=\":\n",
    "            tokens.append(12)\n",
    "        if i == \"\\n\":\n",
    "            tokens.append(13)\n",
    "    return tokens\n",
    "        \n",
    "\n",
    "def decode(tokens: list[int]):\n",
    "    reverse_char_map = {0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9', 11: '+', 12: '=', 13: '\\n'}\n",
    "    return ''.join([reverse_char_map[i] for i in tokens])\n",
    "\n",
    "def convert_to_str(a: int):\n",
    "    n = len(str(a))\n",
    "    return (max_digits-n)*'0' + str(a)\n",
    "\n",
    "def sample_data(split: str = \"train\"): # With replacement\n",
    "    X = torch.zeros(batch_size, context_length).long()\n",
    "    Y = torch.zeros(batch_size, context_length).long()\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        a = random.randint(0,10**(max_digits-1))\n",
    "        b = random.randint(0,10**(max_digits-1))\n",
    "        c = a + b\n",
    "        x = tokenise(f\"{convert_to_str(a)}+{convert_to_str(b)}={convert_to_str(c)[::-1]}\")\n",
    "        y = x[1:] + [13]\n",
    "\n",
    "        X[i, :len(x)] = torch.tensor(x)\n",
    "        Y[i, :len(y)] = torch.tensor(y)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = sample_data(split)\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    \n",
    "    return out[\"train\"], out['val']\n",
    "\n",
    "\n",
    "class Layer(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(model_dim, head_dim)\n",
    "        self.query = nn.Linear(model_dim, head_dim)\n",
    "        self.value = nn.Linear(model_dim, head_dim)\n",
    "        self.proj = nn.Linear(head_dim, model_dim)\n",
    "    \n",
    "    def forward(self, idx):\n",
    "        key = self.key(idx) # (batch, context_length, head_dim)\n",
    "        query = self.query(idx)\n",
    "        value = self.value(idx) # (batch, context_length, head_dim)\n",
    "\n",
    "        attention = (query@torch.transpose(key,1,2))/(math.sqrt(head_dim)) # (batch, context_length, context_length)\n",
    "\n",
    "        attention = torch.tril(attention)\n",
    "\n",
    "        attention = attention.masked_fill(attention == 0, -inf)\n",
    "\n",
    "        attention = F.softmax(attention,-1) # probs along context_length sum to 1\n",
    "\n",
    "        attention_value = attention@value  # (batch, context_length, head_dim)\n",
    "\n",
    "        return self.proj(attention_value)  # (batch, context_length, model_dim)\n",
    "    \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(model_dim, 4*model_dim), nn.Linear(4*model_dim, model_dim))\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, idx):\n",
    "        logits = self.layers(idx)\n",
    "        return self.relu(logits)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, model_dim)\n",
    "        self.pos_embedding = nn.Embedding(context_length, model_dim)\n",
    "        self.attention_layes = nn.ModuleList([AttentionHead() for i in range(n_layers)])\n",
    "        self.mlp_layers = nn.ModuleList([MLP() for i in range(n_layers)])\n",
    "        self.unembed_layer = nn.Linear(model_dim,vocab_size)\n",
    "\n",
    "        self.total_parameters = sum([p.numel() for p in self.parameters()])\n",
    "        print(f\"Model has {self.total_parameters//1000}k params\")\n",
    "\n",
    "\n",
    "    def forward(self, idx, targets = None):\n",
    "        # idx -> [1,2,0,3..] (batch, context_length)\n",
    "\n",
    "        # for p in range(idx.shape[0]):\n",
    "        #     print([decode(idx[p].tolist()), decode(targets[p].tolist())])\n",
    "\n",
    "        input_sequence_length = idx.shape[-1]\n",
    "\n",
    "        residual_stream = self.token_embedding(idx)  # (batch, context_length, model_dim)\n",
    "        residual_stream = residual_stream + self.pos_embedding(torch.tensor([i for i in range(input_sequence_length)])) # Pos embedding will be # (context_length, model_dim)\n",
    "        \n",
    "        for i in range(n_layers):\n",
    "            residual_stream = residual_stream + self.attention_layes[i](residual_stream)\n",
    "            residual_stream = residual_stream + self.mlp_layers[i](residual_stream)\n",
    "\n",
    "        residual_stream = self.unembed_layer(residual_stream) # (batch, context_length, vocab_size)\n",
    "        if targets is None:\n",
    "            return residual_stream\n",
    "        \n",
    "        \n",
    "        # residual_stream = residual_stream[:,9:,:]\n",
    "        # targets = targets[:,9:]\n",
    "\n",
    "\n",
    "        (x,y,z) = residual_stream.shape\n",
    "\n",
    "        # print(residual_stream.shape, targets.shape)\n",
    "\n",
    "        starting_index_to_calculate_loss = 2*max_digits+1\n",
    "        residual_stream = residual_stream[:,starting_index_to_calculate_loss:,:]\n",
    "        targets= targets[:,starting_index_to_calculate_loss:]\n",
    "\n",
    "        (x,y,z) = residual_stream.shape\n",
    "\n",
    "        loss = F.cross_entropy(residual_stream.reshape(x*y,z), targets.reshape(x*y))\n",
    "        return residual_stream, loss\n",
    "    \n",
    "model = Transformer()\n",
    "\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    model = Transformer()\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    model.eval()\n",
    "\n",
    "train_loss,val_loss = estimate_loss()\n",
    "print(f\"Initial training loss: {train_loss}, val loss: {val_loss}\")\n",
    "\n",
    "loss_value = []\n",
    "val_loss_value = []\n",
    "iters = []\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "step_value = max_iters/20\n",
    "start_time = time.time()\n",
    "for iter in range(max_iters):\n",
    "    X,Y= sample_data() # (B, context_length)\n",
    "    logits, loss = model(X, Y)  # (B, context_length, vocab_size)\n",
    "    if iter%step_value ==0:\n",
    "        train_loss,val_loss = estimate_loss()\n",
    "        iters.append(iter)\n",
    "        loss_value.append(train_loss)\n",
    "        val_loss_value.append(val_loss)\n",
    "        print(f\"iter:{iter} training loss: {train_loss}, val loss: {val_loss}\")\n",
    "\n",
    "    \n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "end_time = time.time()\n",
    "print(f\"Took {end_time-start_time}s for {max_iters} epochs\")\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(iters,loss_value, color='blue', label=\"Training\")\n",
    "plt.plot(iters, val_loss_value, \"red\", label = \"validation\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 95.6%\n"
     ]
    }
   ],
   "source": [
    "def generate_text(input: str):\n",
    "    max_tokens = max_digits\n",
    "    input_tokens = torch.tensor(tokenise(input))\n",
    "    #print(input, end='')\n",
    "    out = []\n",
    "    \n",
    "    for i in range(max_tokens):\n",
    "        now = model(input_tokens.unsqueeze(0))[0][-1]\n",
    "        now = F.softmax(now, dim= 0)\n",
    "        token = torch.multinomial(now,1).item()\n",
    "        input_tokens = torch.tensor(input_tokens.tolist() + [token])\n",
    "        text = decode([token])\n",
    "        #print(text, end='')\n",
    "        out.append(text)\n",
    "        input_tokens = input_tokens[-context_length:]\n",
    "    return out\n",
    "                \n",
    "true = 0\n",
    "tot = 0\n",
    "\n",
    "for i in range(1000):\n",
    "    tot+=1\n",
    "    a = random.randint(0,10**(max_digits-1))\n",
    "    b = random.randint(0,10**(max_digits-1))\n",
    "    expected = a + b\n",
    "    input = f\"{convert_to_str(a)}+{convert_to_str(b)}=\"\n",
    "    sum = int(''.join(generate_text(input)[::-1]))\n",
    "    if sum == expected:\n",
    "        true+=1\n",
    "\n",
    "print(f\"Accuracy {true*100/tot}%\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"models/addition_1M_95_accuracy.bin\"\n",
    "torch.save(model.state_dict(), PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6', '2', '4', '6', '4', '0']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(\"012301+034125=\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
